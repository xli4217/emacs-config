;; Object agents/
;; SEMANTICDB Tags save file
(semanticdb-project-database-file "agents/"
  :tables
  (list
    (semanticdb-table "ppo_agent.py"
      :major-mode 'python-mode
      :tags 
        '( ("__future__" include nil nil [681 719])
            ("__future__" include nil nil [720 757])
            ("__future__" include nil nil [758 789])
            ("tensorforce" include nil nil [791 831])
            ("tensorforce.agents" include nil nil [832 873])
            ("tensorforce.models" include nil nil [874 921])
            ("PPOAgent" type
               (:documentation "
    Proximal Policy Optimization agent ([Schulman et al., 2017]
    (https://openai-public.s3-us-west-2.amazonaws.com/blog/2017-07/ppo/ppo-arxiv.pdf).
    "
                :superclasses ("BatchAgent")
                :members 
                  ( ("__init__" function
                       (:suite 
                          ( ("\"\"\"
        Creates a proximal policy optimization agent (PPO), ([Schulman et al., 2017]
        (https://openai-public.s3-us-west-2.amazonaws.com/blog/2017-07/ppo/ppo-arxiv.pdf).

        Args:
            states_spec: Dict containing at least one state definition. In the case of a single state,
               keys `shape` and `type` are necessary. For multiple states, pass a dict of dicts where each state
               is a dict itself with a unique name as its key.
            actions_spec: Dict containing at least one action definition. Actions have types and either `num_actions`
                for discrete actions or a `shape` for continuous actions. Consult documentation and tests for more.
            network_spec: List of layers specifying a neural network via layer types, sizes and optional arguments
                such as activation or regularisation. Full examples are in the examples/configs folder.
            device: Device string specifying model device.
            session_config: optional tf.ConfigProto with additional desired session configurations
            scope: TensorFlow scope, defaults to agent name (e.g. `dqn`).
            saver_spec: Dict specifying automated saving. Use `directory` to specify where checkpoints are saved. Use
                either `seconds` or `steps` to specify how often the model should be saved. The `load` flag specifies
                if a model is initially loaded (set to True) from a file `file`.
            summary_spec: Dict specifying summaries for TensorBoard. Requires a 'directory' to store summaries, `steps`
                or `seconds` to specify how often to save summaries, and a list of `labels` to indicate which values
                to export, e.g. `losses`, `variables`. Consult neural network class and model for all available labels.
            distributed_spec: Dict specifying distributed functionality. Use `parameter_server` and `replica_model`
                Boolean flags to indicate workers and parameter servers. Use a `cluster_spec` key to pass a TensorFlow
                cluster spec.
            discount: Float specifying reward discount factor.
            variable_noise: Experimental optional parameter specifying variable noise (NoisyNet).
            states_preprocessing_spec: Optional list of states preprocessors to apply to state  
                (e.g. `image_resize`, `grayscale`).
            explorations_spec: Optional dict specifying action exploration type (epsilon greedy  
                or Gaussian noise).
            reward_preprocessing_spec: Optional dict specifying reward preprocessing.
            distributions_spec: Optional dict specifying action distributions to override default distribution choices.
                Must match action names.
            entropy_regularization: Optional positive float specifying an entropy regularization value.
            baseline_mode: String specifying baseline mode, `states` for a separate baseline per state, `network`
                for sharing parameters with the training network.
            baseline: Optional dict specifying baseline type (e.g. `mlp`, `cnn`), and its layer sizes. Consult
                examples/configs for full example configurations.
            baseline_optimizer: Optional dict specifying an optimizer and its parameters for the baseline following
                the same conventions as the main optimizer.
            gae_lambda: Optional float specifying lambda parameter for generalized advantage estimation.
            batched_observe: Optional int specifying how many observe calls are batched into one session run.
                Without batching, throughput will be lower because every `observe` triggers a session invocation to
                update rewards in the graph.
            batch_size: Int specifying number of samples collected via `observe` before an update is executed.
            keep_last_timestep: Boolean flag specifying whether last sample is kept, default True.
            likelihood_ratio_clipping: Optional clipping of likelihood ratio between old and new policy.
            step_optimizer: Optimizer dict specification for optimizer used in each PPO update step, defaults to
                Adam if None.
            optimization_steps: Int specifying number of optimization steps to execute on the collected batch using
                the step optimizer.                `
        \"\"\"" code nil (reparse-symbol indented_block_body) [1988 6428])
                            ("if" code nil (reparse-symbol indented_block_body) [6437 6526])
                            ("self" variable nil (reparse-symbol indented_block_body) [6535 6567])
                            ("self" variable nil (reparse-symbol indented_block_body) [6576 6596])
                            ("self" variable nil (reparse-symbol indented_block_body) [6605 6641])
                            ("self" variable nil (reparse-symbol indented_block_body) [6650 6668])
                            ("self" variable nil (reparse-symbol indented_block_body) [6677 6705])
                            ("self" variable nil (reparse-symbol indented_block_body) [6714 6746])
                            ("self" variable nil (reparse-symbol indented_block_body) [6755 6795])
                            ("self" variable nil (reparse-symbol indented_block_body) [6804 6828])
                            ("self" variable nil (reparse-symbol indented_block_body) [6837 6873])
                            ("self" variable nil (reparse-symbol indented_block_body) [6882 6940])
                            ("self" variable nil (reparse-symbol indented_block_body) [6949 6991])
                            ("self" variable nil (reparse-symbol indented_block_body) [7000 7058])
                            ("self" variable nil (reparse-symbol indented_block_body) [7067 7111])
                            ("self" variable nil (reparse-symbol indented_block_body) [7120 7172])
                            ("self" variable nil (reparse-symbol indented_block_body) [7181 7215])
                            ("self" variable nil (reparse-symbol indented_block_body) [7224 7248])
                            ("self" variable nil (reparse-symbol indented_block_body) [7257 7301])
                            ("self" variable nil (reparse-symbol indented_block_body) [7310 7338])
                            ("self" variable nil (reparse-symbol indented_block_body) [7347 7405])
                            ("if" code nil (reparse-symbol indented_block_body) [7415 7555])
                            ("self" variable nil (reparse-symbol indented_block_body) [7564 7706])
                            ("super" code nil (reparse-symbol indented_block_body) [7716 7963]))                          
                        :parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [1146 1150])
                            ("states_spec" variable nil (reparse-symbol function_parameters) [1160 1171])
                            ("actions_spec" variable nil (reparse-symbol function_parameters) [1181 1193])
                            ("network_spec" variable nil (reparse-symbol function_parameters) [1203 1215])
                            ("device" variable nil (reparse-symbol function_parameters) [1225 1231])
                            ("session_config" variable nil (reparse-symbol function_parameters) [1246 1260])
                            ("scope" variable nil (reparse-symbol function_parameters) [1275 1280])
                            ("saver_spec" variable nil (reparse-symbol function_parameters) [1296 1306])
                            ("summary_spec" variable nil (reparse-symbol function_parameters) [1321 1333])
                            ("distributed_spec" variable nil (reparse-symbol function_parameters) [1348 1364])
                            ("discount" variable nil (reparse-symbol function_parameters) [1379 1387])
                            ("variable_noise" variable nil (reparse-symbol function_parameters) [1402 1416])
                            ("states_preprocessing_spec" variable nil (reparse-symbol function_parameters) [1431 1456])
                            ("explorations_spec" variable nil (reparse-symbol function_parameters) [1471 1488])
                            ("reward_preprocessing_spec" variable nil (reparse-symbol function_parameters) [1503 1528])
                            ("distributions_spec" variable nil (reparse-symbol function_parameters) [1543 1561])
                            ("entropy_regularization" variable nil (reparse-symbol function_parameters) [1576 1598])
                            ("baseline_mode" variable nil (reparse-symbol function_parameters) [1613 1626])
                            ("baseline" variable nil (reparse-symbol function_parameters) [1641 1649])
                            ("baseline_optimizer" variable nil (reparse-symbol function_parameters) [1664 1682])
                            ("gae_lambda" variable nil (reparse-symbol function_parameters) [1697 1707])
                            ("batched_observe" variable nil (reparse-symbol function_parameters) [1722 1737])
                            ("batch_size" variable nil (reparse-symbol function_parameters) [1752 1762])
                            ("keep_last_timestep" variable nil (reparse-symbol function_parameters) [1777 1795])
                            ("likelihood_ratio_clipping" variable nil (reparse-symbol function_parameters) [1810 1835])
                            ("step_optimizer" variable nil (reparse-symbol function_parameters) [1850 1864])
                            ("optimization_steps" variable nil (reparse-symbol function_parameters) [1879 1897]))                          
                        :documentation "
        Creates a proximal policy optimization agent (PPO), ([Schulman et al., 2017]
        (https://openai-public.s3-us-west-2.amazonaws.com/blog/2017-07/ppo/ppo-arxiv.pdf).

        Args:
            states_spec: Dict containing at least one state definition. In the case of a single state,
               keys `shape` and `type` are necessary. For multiple states, pass a dict of dicts where each state
               is a dict itself with a unique name as its key.
            actions_spec: Dict containing at least one action definition. Actions have types and either `num_actions`
                for discrete actions or a `shape` for continuous actions. Consult documentation and tests for more.
            network_spec: List of layers specifying a neural network via layer types, sizes and optional arguments
                such as activation or regularisation. Full examples are in the examples/configs folder.
            device: Device string specifying model device.
            session_config: optional tf.ConfigProto with additional desired session configurations
            scope: TensorFlow scope, defaults to agent name (e.g. `dqn`).
            saver_spec: Dict specifying automated saving. Use `directory` to specify where checkpoints are saved. Use
                either `seconds` or `steps` to specify how often the model should be saved. The `load` flag specifies
                if a model is initially loaded (set to True) from a file `file`.
            summary_spec: Dict specifying summaries for TensorBoard. Requires a 'directory' to store summaries, `steps`
                or `seconds` to specify how often to save summaries, and a list of `labels` to indicate which values
                to export, e.g. `losses`, `variables`. Consult neural network class and model for all available labels.
            distributed_spec: Dict specifying distributed functionality. Use `parameter_server` and `replica_model`
                Boolean flags to indicate workers and parameter servers. Use a `cluster_spec` key to pass a TensorFlow
                cluster spec.
            discount: Float specifying reward discount factor.
            variable_noise: Experimental optional parameter specifying variable noise (NoisyNet).
            states_preprocessing_spec: Optional list of states preprocessors to apply to state  
                (e.g. `image_resize`, `grayscale`).
            explorations_spec: Optional dict specifying action exploration type (epsilon greedy  
                or Gaussian noise).
            reward_preprocessing_spec: Optional dict specifying reward preprocessing.
            distributions_spec: Optional dict specifying action distributions to override default distribution choices.
                Must match action names.
            entropy_regularization: Optional positive float specifying an entropy regularization value.
            baseline_mode: String specifying baseline mode, `states` for a separate baseline per state, `network`
                for sharing parameters with the training network.
            baseline: Optional dict specifying baseline type (e.g. `mlp`, `cnn`), and its layer sizes. Consult
                examples/configs for full example configurations.
            baseline_optimizer: Optional dict specifying an optimizer and its parameters for the baseline following
                the same conventions as the main optimizer.
            gae_lambda: Optional float specifying lambda parameter for generalized advantage estimation.
            batched_observe: Optional int specifying how many observe calls are batched into one session run.
                Without batching, throughput will be lower because every `observe` triggers a session invocation to
                update rewards in the graph.
            batch_size: Int specifying number of samples collected via `observe` before an update is executed.
            keep_last_timestep: Boolean flag specifying whether last sample is kept, default True.
            likelihood_ratio_clipping: Optional clipping of likelihood ratio between old and new policy.
            step_optimizer: Optimizer dict specification for optimizer used in each PPO update step, defaults to
                Adam if None.
            optimization_steps: Int specifying number of optimization steps to execute on the collected batch using
                the step optimizer.                `
        "
                        :constructor-flag t)
                        (reparse-symbol indented_block_body) [1124 7964])
                    ("initialize_model" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [7990 7994]))                          )
                        (reparse-symbol indented_block_body) [7969 9099]))                  
                :type "class")
                nil [924 9099]))          
      :file "ppo_agent.py"
      :pointmax 9099
      :fsize 9098
      :lastmodtime '(23098 62796 444858 228000)
      :unmatched-syntax nil)
    (semanticdb-table "agent.py"
      :major-mode 'python-mode
      :tags 
        '( ("__future__" include nil nil [681 719])
            ("__future__" include nil nil [720 757])
            ("__future__" include nil nil [758 789])
            ("copy" include nil nil [791 816])
            ("numpy" include nil nil [818 836])
            ("inspect" include nil nil [837 851])
            ("tensorforce" include nil nil [853 899])
            ("tensorforce.agents" include nil nil [900 925])
            ("tensorforce.meta_parameter_recorder" include nil nil [926 995])
            ("Agent" type
               (:documentation "
    Basic Reinforcement learning agent. An agent encapsulates execution logic
    of a particular reinforcement learning algorithm and defines the external interface
    to the environment.

    The agent hence acts as an intermediate layer between environment
    and backend execution (value function or policy updates).

    "
                :superclasses ("object")
                :members 
                  ( ("__init__" function
                       (:suite 
                          ( ("\"\"\"
        Initializes the reinforcement learning agent.

        Args:
            states_spec: Dict containing at least one state definition. In the case of a single state,
               keys `shape` and `type` are necessary. For multiple states, pass a dict of dicts where each state
               is a dict itself with a unique name as its key.
            actions_spec: Dict containing at least one action definition. Actions have types and either `num_actions`
                for discrete actions or a `shape` for continuous actions. Consult documentation and tests for more.
            batched_observe: Optional int specifying how many observe calls are batched into one session run.
                Without batching, throughput will be lower because every `observe` triggers a session invocation to
                update rewards in the graph.
        \"\"\"" code nil (reparse-symbol indented_block_body) [1474 2342])
                            ("self" variable nil (reparse-symbol indented_block_body) [2352 2396])
                            ("if" code nil (reparse-symbol indented_block_body) [2405 2477])
                            ("self" variable nil (reparse-symbol indented_block_body) [2486 2526])
                            ("for" code nil (reparse-symbol indented_block_body) [2535 2837])
                            ("self" variable nil (reparse-symbol indented_block_body) [2887 2912])
                            ("self" variable nil (reparse-symbol indented_block_body) [2921 2966])
                            ("if" code nil (reparse-symbol indented_block_body) [2975 3051])
                            ("self" variable nil (reparse-symbol indented_block_body) [3059 3101])
                            ("for" code nil (reparse-symbol indented_block_body) [3111 3858])
                            ("if" code nil (reparse-symbol indented_block_body) [3946 4105])
                            ("self" variable nil (reparse-symbol indented_block_body) [4115 4146])
                            ("if" code nil (reparse-symbol indented_block_body) [4257 4997])
                            ("self" variable nil (reparse-symbol indented_block_body) [5114 5150])
                            ("self" variable nil (reparse-symbol indented_block_body) [5222 5260])
                            ("if" code nil (reparse-symbol indented_block_body) [5269 5390])
                            ("self" code nil (reparse-symbol indented_block_body) [5399 5411]))                          
                        :parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [1386 1390])
                            ("states_spec" variable nil (reparse-symbol function_parameters) [1400 1411])
                            ("actions_spec" variable nil (reparse-symbol function_parameters) [1421 1433])
                            ("batched_observe" variable nil (reparse-symbol function_parameters) [1443 1458]))                          
                        :documentation "
        Initializes the reinforcement learning agent.

        Args:
            states_spec: Dict containing at least one state definition. In the case of a single state,
               keys `shape` and `type` are necessary. For multiple states, pass a dict of dicts where each state
               is a dict itself with a unique name as its key.
            actions_spec: Dict containing at least one action definition. Actions have types and either `num_actions`
                for discrete actions or a `shape` for continuous actions. Consult documentation and tests for more.
            batched_observe: Optional int specifying how many observe calls are batched into one session run.
                Without batching, throughput will be lower because every `observe` triggers a session invocation to
                update rewards in the graph.
        "
                        :constructor-flag t)
                        (reparse-symbol indented_block_body) [1364 5412])
                    ("__str__" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [5429 5433]))                          )
                        (reparse-symbol indented_block_body) [5417 5480])
                    ("close" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [5495 5499]))                          )
                        (reparse-symbol indented_block_body) [5485 5529])
                    ("initialize_model" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [5555 5559]))                          
                        :documentation "
        Creates the model for the respective agent based on specifications given by user. This is a separate
        call after constructing the agent because the agent constructor has to perform a number of checks
        on the specs first, sometimes adjusting them e.g. by converting to a dict.
        ")
                        (reparse-symbol indented_block_body) [5534 5918])
                    ("reset" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [5933 5937]))                          
                        :documentation "
        Reset the agent to its initial state on episode start. Updates internal episode and  
        timestep counter, internal states,  and resets preprocessors.
        ")
                        (reparse-symbol indented_block_body) [5923 6259])
                    ("act" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [6425 6429])
                            ("states" variable nil (reparse-symbol function_parameters) [6431 6437])
                            ("deterministic" variable nil (reparse-symbol function_parameters) [6439 6452]))                          
                        :documentation "
        Return action(s) for given state(s). States preprocessing and exploration are applied if  
        configured accordingly.

        Args:
            states: One state (usually a value tuple) or dict of states if multiple states are expected.
            deterministic: If true, no exploration and sampling is applied.
        Returns:
            Scalar value of the action or dict of multiple actions the agent wants to execute.

        ")
                        (reparse-symbol indented_block_body) [6417 7564])
                    ("observe" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [7581 7585])
                            ("terminal" variable nil (reparse-symbol function_parameters) [7587 7595])
                            ("reward" variable nil (reparse-symbol function_parameters) [7597 7603]))                          
                        :documentation "
        Observe experience from the environment to learn from. Optionally preprocesses rewards
        Child classes should call super to get the processed reward
        EX: terminal, reward = super()...

        Args:
            terminal: boolean indicating if the episode terminated after the observation.
            reward: scalar reward that resulted from executing the action.
        ")
                        (reparse-symbol indented_block_body) [7569 8878])
                    ("should_stop" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [8899 8903]))                          )
                        (reparse-symbol indented_block_body) [8883 8964])
                    ("last_observation" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [8990 8994]))                          )
                        (reparse-symbol indented_block_body) [8969 9239])
                    ("save_model" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [9259 9263])
                            ("directory" variable nil (reparse-symbol function_parameters) [9265 9274])
                            ("append_timestep" variable nil (reparse-symbol function_parameters) [9281 9296]))                          
                        :documentation "
        Save TensorFlow model. If no checkpoint directory is given, the model's default saver  
        directory is used. Optionally appends current timestep to prevent overwriting previous  
        checkpoint files. Turn off to be able to load model from the same given path argument as  
        given here.

        Args:
            directory: Optional checkpoint directory.
            use_global_step:  Appends the current timestep to the checkpoint file if true.
            If this is set to True, the load path must include the checkpoint timestep suffix.  
            For example, if stored to models/ and set to true, the exported file will be of the  
            form models/model.ckpt-X where X is the last timestep saved. The load path must  
            precisely match this file name. If this option is turned off, the checkpoint will  
            always overwrite the file specified in path and the model can always be loaded under  
            this path.

        Returns:
            Checkpoint path were the model was saved.
        ")
                        (reparse-symbol indented_block_body) [9244 10464])
                    ("restore_model" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [10487 10491])
                            ("directory" variable nil (reparse-symbol function_parameters) [10493 10502])
                            ("file" variable nil (reparse-symbol function_parameters) [10509 10513]))                          
                        :documentation "
        Restore TensorFlow model. If no checkpoint file is given, the latest checkpoint is  
        restored. If no checkpoint directory is given, the model's default saver directory is  
        used (unless file specifies the entire path).

        Args:
            directory: Optional checkpoint directory.
            file: Optional checkpoint file, or path if directory not given.
        ")
                        (reparse-symbol indented_block_body) [10469 10992])
                    ("from_spec" function
                       (:typemodifiers ("static")
                        :decorators 
                          ( ("staticmethod" function (:type "decorator") nil nil))                          
                        :arguments 
                          ( ("spec" variable nil (reparse-symbol function_parameters) [11029 11033])
                            ("kwargs" variable nil (reparse-symbol function_parameters) [11035 11041]))                          
                        :documentation "
        Creates an agent from a specification dict.
        ")
                        (reparse-symbol indented_block_body) [10997 11330]))                  
                :type "class")
                nil [998 11330]))          
      :file "agent.py"
      :pointmax 11330
      :fsize 11329
      :lastmodtime '(23098 62796 444858 228000)
      :unmatched-syntax nil))
  :file "!home!burobotics!Xiao!rlfps!external_libs!tensorforce!tensorforce!agents!semantic.cache"
  :semantic-tag-version "2.0"
  :semanticdb-version "2.2")
