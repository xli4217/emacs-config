;; Object simple_example/
;; SEMANTICDB Tags save file
(semanticdb-project-database-file "simple_example/"
  :tables
  (list
    (semanticdb-table "get_rewards.py"
      :major-mode 'python-mode
      :tags 
        '( ("os" include nil nil [1 10])
            ("numpy" include nil nil [11 29])
            ("rlfps.rewards.TL.automata_guided_reward.fsa_reward" include nil nil [30 102])
            ("external_libs.lomap.classes" include nil nil [103 146])
            ("rlfps.automata_utils.plot_dynamic_graph" include nil nil [147 218])
            ("baxter_example.simple_example.baxter_tl.scripts.config" include nil nil [286 376])
            ("baxter_example.simple_example.baxter_tl.scripts.config" include nil nil [377 456])
            ("table_origin" variable nil nil [529 560])
            ("table_width" variable nil nil [614 632])
            ("table_length" variable nil nil [633 652])
            ("table_thickness" variable nil nil [653 672])
            ("table_planar_ub" variable nil nil [674 755])
            ("table_planar_lb" variable nil nil [756 837])
            ("print" code nil nil [839 880])
            ("red_button_xy" variable nil nil [976 1048])
            ("red_button_r" code nil nil [1049 1122])
            ("blue_button_xy" variable nil nil [1124 1198])
            ("blue_button_r" code nil nil [1199 1274])
            ("shrink" variable nil nil [1277 1290])
            ("red_plate_xy" variable nil nil [1292 1362])
            ("red_plate_width" code nil nil [1363 1445])
            ("red_plate_length" code nil nil [1456 1539])
            ("blue_plate_xy" variable nil nil [1551 1623])
            ("blue_plate_width" code nil nil [1624 1708])
            ("blue_plate_length" code nil nil [1719 1804])
            ("black_plate_xy" variable nil nil [1816 1890])
            ("black_plate_width" code nil nil [1891 1977])
            ("black_plate_length" code nil nil [1988 2074])
            ("hand_in_red" function (:arguments 
              ( ("s" variable nil (reparse-symbol function_parameters) [2102 2103]))              ) nil [2086 2253])
            ("hand_in_blue" function (:arguments 
              ( ("s" variable nil (reparse-symbol function_parameters) [2271 2272]))              ) nil [2254 2425])
            ("hand_in_black" function (:arguments 
              ( ("s" variable nil (reparse-symbol function_parameters) [2444 2445]))              ) nil [2426 2813])
            ("get_robustness" function (:arguments 
              ( ("s" variable nil (reparse-symbol function_parameters) [2869 2870])
                ("s_type" variable nil (reparse-symbol function_parameters) [2871 2877])
                ("instruction" variable nil (reparse-symbol function_parameters) [2878 2889]))              ) nil [2850 7151])
            ("TL2_robustness_func" function (:arguments 
              ( ("traj" variable nil (reparse-symbol function_parameters) [7204 7208])
                ("formula_idx" variable nil (reparse-symbol function_parameters) [7210 7221]))              ) nil [7180 10583])
            ("heuristic_reward2" function (:arguments 
              ( ("s" variable nil (reparse-symbol function_parameters) [10614 10615])
                ("hr" variable nil (reparse-symbol function_parameters) [10617 10619])
                ("hb" variable nil (reparse-symbol function_parameters) [10621 10623])
                ("hbl" variable nil (reparse-symbol function_parameters) [10625 10628])
                ("formula_idx" variable nil (reparse-symbol function_parameters) [10630 10641]))              ) nil [10592 13074])
            ("reward_dict" variable nil nil [13082 13968])
            ("'''
s = [(x,y)_hand, (x,y)_red_box]
astate takes 0,1,2,3

formula_idx = 1: phi_1 = (hand_in_red_button -> F red_box_in_red_plate) ^  (!hand_in_red_button -> F red_box_in_black_plate)

formula_idx = 2: phi_1 = (hand_in_red_button -> F red_box_in_red_plate) ^  (!(hand_in_red_button v hand_in_black_plate) -> F red_box_in_black_plate)

formula_idx = 3: phi_2 = (hand_in_blue_button -> F blue_box_in_blue_plate) ^  (!(hand_in_blue_button v hand_in_black_plate) -> F blue_box_in_black_plate)

formula_idx = 4: phi_3 = hand_in_black_plate -> F( red_box_in_blue_plate)

formula_idx = 5: phi_4 = hand_in_black_plate -> F( blue_box_in_red_plate)

formula_idx = 6: phi_5 = phi_2 ^ phi_3

formula_idx = 7: phi_7 = phi_2 ^ phi_3  ^ phi_4 ^ phi_5
    
'''" code nil nil [13971 14714])
            ("spec1" variable nil nil [14716 14758])
            ("spec2" variable nil nil [14759 14810])
            ("spec3" variable nil nil [14811 14862])
            ("spec4" variable nil nil [14863 14887])
            ("spec5" variable nil nil [14888 14912])
            ("spec6" code nil nil [14913 14943])
            ("spec7" code nil nil [14944 15008])
            ("specs" variable nil nil [15010 15061])
            ("get_reward_and_spaces" function (:arguments 
              ( ("formula_idx" variable nil (reparse-symbol function_parameters) [15091 15102])
                ("reward_type" variable nil (reparse-symbol function_parameters) [15104 15115]))              ) nil [15065 17411]))          
      :file "get_rewards.py"
      :pointmax 17411
      :fsize 17410
      :lastmodtime '(23044 54161 628709 598000)
      :unmatched-syntax nil)
    (semanticdb-table "config.py"
      :file "config.py"
      :fsize 1944
      :lastmodtime '(23044 54161 556710 149000))
    (semanticdb-table "deploy_model.py"
      :major-mode 'python-mode
      :tags 
        '( ("numpy" include nil nil [1 19])
            ("tensorflow" include nil nil [20 43])
            ("os" include nil nil [44 53])
            ("mlp_policy" include nil nil [54 86])
            ("pick_n_place_env" include nil nil [87 129])
            ("DeployModel" type
               (:members 
                  ( ("__init__" function
                       (:suite 
                          ( ("self" variable nil (reparse-symbol indented_block_body) [231 275])
                            ("if" code nil (reparse-symbol indented_block_body) [284 694])
                            ("self" variable nil (reparse-symbol indented_block_body) [715 769])
                            ("self" variable nil (reparse-symbol indented_block_body) [778 832])
                            ("self" variable nil (reparse-symbol indented_block_body) [842 898])
                            ("self" variable nil (reparse-symbol indented_block_body) [907 963])
                            ("self" variable nil (reparse-symbol indented_block_body) [973 1008])
                            ("self" variable nil (reparse-symbol indented_block_body) [1017 1051]))                          
                        :parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [170 174])
                            ("config" variable nil (reparse-symbol function_parameters) [176 182])
                            ("model_dir" variable nil (reparse-symbol function_parameters) [184 193]))                          
                        :constructor-flag t)
                        (reparse-symbol indented_block_body) [157 1052])
                    ("get_action" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [1072 1076])
                            ("obs" variable nil (reparse-symbol function_parameters) [1078 1081]))                          )
                        (reparse-symbol indented_block_body) [1057 1296]))                  
                :type "class")
                nil [131 1296])
            ("if" code nil nil [1337 1530]))          
      :file "deploy_model.py"
      :pointmax 8286
      :fsize 8285
      :lastmodtime '(23044 54161 556710 149000)
      :unmatched-syntax nil)
    (semanticdb-table "mlp_policy.py"
      :file "mlp_policy.py"
      :fsize 10158
      :lastmodtime '(23044 54161 628709 598000))
    (semanticdb-table "pick_n_place_env.py"
      :file "pick_n_place_env.py"
      :fsize 4641
      :lastmodtime '(23044 54802 915760 162000))
    (semanticdb-table "ppo.py"
      :major-mode 'python-mode
      :tags 
        '( ("\"\"\"
PPO: Proximal Policy Optimization
Written by Patrick Coady (pat-coady.github.io)
PPO uses a loss function and gradient descent to approximate
Trust Region Policy Optimization (TRPO). See these papers for
details:

TRPO / PPO:
https://arxiv.org/pdf/1502.05477.pdf (Schulman et al., 2016)

Distributed PPO:
https://arxiv.org/abs/1707.02286 (Heess et al., 2017)

Generalized Advantage Estimation:
https://arxiv.org/pdf/1506.02438.pdf

And, also, this GitHub repo which was helpful to me during
implementation:
https://github.com/joschu/modular_rl

\"\"\"" code nil nil [1 553])
            ("gym" include nil nil [555 565])
            ("baxter_example.simple_example" include nil nil [566 602])
            ("gym" include nil nil [603 627])
            ("baxter_example.simple_example.mlp_policy" include nil nil [628 690])
            ("baxter_example.simple_example.mlp_value" include nil nil [691 751])
            ("scipy.signal" include nil nil [752 771])
            ("utils" include nil nil [772 804])
            ("datetime" include nil nil [805 834])
            ("os" include nil nil [835 844])
            ("signal" include nil nil [845 858])
            ("config" include nil nil [859 884])
            ("rlfps.misc.logger" include nil nil [885 919])
            ("numpy" include nil nil [920 938])
            ("tensorflow" include nil nil [939 962])
            ("logger" code nil nil [965 1036])
            ("GracefulKiller" type
               (:documentation " Gracefully exit program on CTRL-C "
                :members 
                  ( ("__init__" function
                       (:suite 
                          ( ("self" variable nil (reparse-symbol indented_block_body) [1138 1159])
                            ("signal" code nil (reparse-symbol indented_block_body) [1168 1218])
                            ("signal" code nil (reparse-symbol indented_block_body) [1227 1278]))                          
                        :parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [1123 1127]))                          
                        :constructor-flag t)
                        (reparse-symbol indented_block_body) [1110 1279])
                    ("exit_gracefully" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [1304 1308])
                            ("signum" variable nil (reparse-symbol function_parameters) [1310 1316])
                            ("frame" variable nil (reparse-symbol function_parameters) [1318 1323]))                          )
                        (reparse-symbol indented_block_body) [1284 1355]))                  
                :type "class")
                nil [1038 1355])
            ("init_gym" function (:documentation "
    Initialize gym environment, return dimension of observation
    and action spaces.
    Args:
    env_name: str environment name (e.g. \"Humanoid-v1\")
    Returns: 3-tuple
    gym environment (object)
    number of observation dimensions (int)
    number of action dimensions (int)
    ") nil [1357 1830])
            ("run_episode" function
               (:documentation " Run single episode with option to animate
    Args:
    env: ai gym environment
    policy: policy object with sample() method
    scaler: scaler object, used to scale/offset each observation dimension
    to a similar range
    animate: boolean, True uses env.render() method to animate episode
    
    Returns: 4-tuple of NumPy arrays
    observes: shape = (episode len, obs_dim)
    actions: shape = (episode len, act_dim)
    rewards: shape = (episode len,)
    unscaled_obs: useful for training scaler, shape = (episode len, obs_dim)
    "
                :arguments 
                  ( ("env" variable nil (reparse-symbol function_parameters) [1848 1851])
                    ("policy" variable nil (reparse-symbol function_parameters) [1853 1859])
                    ("scaler" variable nil (reparse-symbol function_parameters) [1861 1867])
                    ("animate" variable nil (reparse-symbol function_parameters) [1869 1876]))                  )
                nil [1832 3346])
            ("run_policy" function
               (:documentation " Run policy and collect data for a minimum of min_steps and min_episodes
    Args:
             env: a gym environment
             policy: policy object with sample() method
             scaler: scaler object, used to scale/offset each observation dimension
                 to a similar range
             episodes: total episodes to run
         Returns: list of trajectory dictionaries, list length = number of episodes
             'observes' : NumPy array of states from episode
             'actions' : NumPy array of actions from episode
             'rewards' : NumPy array of (un-discounted) rewards from episode
             'unscaled_obs' : NumPy array of (un-discounted) rewards from episode
     "
                :arguments 
                  ( ("env" variable nil (reparse-symbol function_parameters) [3362 3365])
                    ("policy" variable nil (reparse-symbol function_parameters) [3367 3373])
                    ("scaler" variable nil (reparse-symbol function_parameters) [3375 3381])
                    ("batch_size" variable nil (reparse-symbol function_parameters) [3383 3393]))                  )
                nil [3347 5435])
            ("discount" function
               (:documentation " Calculate discounted forward sum of a sequence at each point "
                :arguments 
                  ( ("x" variable nil (reparse-symbol function_parameters) [5449 5450])
                    ("gamma" variable nil (reparse-symbol function_parameters) [5452 5457]))                  )
                nil [5436 5602])
            ("add_disc_sum_rew" function
               (:documentation " Adds discounted sum of rewards to all time steps of all trajectories
    Args:
    trajectories: as returned by run_policy()
    gamma: discount

    Returns:
    None (mutates trajectories dictionary to add 'disc_sum_rew')
    "
                :arguments 
                  ( ("trajectories" variable nil (reparse-symbol function_parameters) [5625 5637])
                    ("gamma" variable nil (reparse-symbol function_parameters) [5639 5644]))                  )
                nil [5604 6193])
            ("add_value" function
               (:documentation " Adds estimated value to all time steps of all trajectories
    Args:
    trajectories: as returned by run_policy()
    val_func: object with predict() method, takes observations
    and returns predicted state value
    Returns:
    None (mutates trajectories dictionary to add 'values')
    "
                :arguments 
                  ( ("trajectories" variable nil (reparse-symbol function_parameters) [6209 6221])
                    ("val_func" variable nil (reparse-symbol function_parameters) [6223 6231]))                  )
                nil [6195 6698])
            ("add_gae" function
               (:documentation " Add generalized advantage estimator.
    https://arxiv.org/pdf/1506.02438.pdf
    Args:
    trajectories: as returned by run_policy(), must include 'values'
    key from add_value().
    gamma: reward discount
    lam: lambda (see paper).
    lam=0 : use TD residuals
    lam=1 : A =  Sum Discounted Rewards - V_hat(s)

    Returns:
    None (mutates trajectories dictionary to add 'advantages')
    "
                :arguments 
                  ( ("trajectories" variable nil (reparse-symbol function_parameters) [6712 6724])
                    ("gamma" variable nil (reparse-symbol function_parameters) [6726 6731])
                    ("lam" variable nil (reparse-symbol function_parameters) [6733 6736]))                  )
                nil [6700 7588])
            ("build_train_set" function
               (:documentation "
    Args:
    trajectories: trajectories after processing by add_disc_sum_rew(),
    add_value(), and add_gae()

    Returns: 4-tuple of NumPy arrays
    observes: shape = (N, obs_dim)
    actions: shape = (N, act_dim)
    advantages: shape = (N,)
    disc_sum_rew: shape = (N,)
    "
                :arguments 
                  ( ("trajectories" variable nil (reparse-symbol function_parameters) [7610 7622]))                  )
                nil [7590 8372])
            ("log_batch_stats" function
               (:documentation " Log various batch statistics "
                :arguments 
                  ( ("observes" variable nil (reparse-symbol function_parameters) [8394 8402])
                    ("actions" variable nil (reparse-symbol function_parameters) [8404 8411])
                    ("advantages" variable nil (reparse-symbol function_parameters) [8413 8423])
                    ("disc_sum_rew" variable nil (reparse-symbol function_parameters) [8425 8437])
                    ("iteration" variable nil (reparse-symbol function_parameters) [8439 8448]))                  )
                nil [8374 9546])
            ("train" function
               (:documentation " Main training loop
    Args:
    env_name: OpenAI Gym environment name, e.g. 'Hopper-v1'
    num_episodes: maximum number of episodes to run
    gamma: reward discount factor (float)
    lam: lambda from Generalized Advantage Estimate
    kl_targ: D_KL target for policy update [D_KL(pi_old || pi_new)
    batch_size: number of episodes per policy training batch
    "
                :arguments 
                  ( ("max_iterations" variable nil (reparse-symbol function_parameters) [9562 9576])
                    ("gamma" variable nil (reparse-symbol function_parameters) [9578 9583])
                    ("lam" variable nil (reparse-symbol function_parameters) [9585 9588])
                    ("batch_size" variable nil (reparse-symbol function_parameters) [9590 9600]))                  )
                nil [9552 11693])
            ("if" code nil nil [11695 12019]))          
      :file "ppo.py"
      :pointmax 12023
      :fsize 12022
      :lastmodtime '(23044 54161 628709 598000)
      :unmatched-syntax nil)
    (semanticdb-table "mlp_value.py"
      :file "mlp_value.py"
      :fsize 4729
      :lastmodtime '(23044 54161 628709 598000))
    (semanticdb-table "utils.py"
      :file "utils.py"
      :fsize 4103
      :lastmodtime '(23044 54161 628709 598000))
    (semanticdb-table "test.py"
      :major-mode 'python-mode
      :tags 
        '( ("numpy" include nil nil [1 19])
            ("'''
    s = [(x,y)_hand, (x,y)_red_box]
    astate takes 0,1,2,3
    formula_idx = 1: phi_1 = (hand_in_red_button -> F red_box_in_red_plate) ^  (!(hand_in_red_button v hand_in_black_plate) -> F red_box_in_black_plate)
    formula_idx = 2: phi_2 = (hand_in_blue_button -> F blue_box_in_blue_plate) ^  (!(hand_in_blue_button v hand_in_black_plate) -> F blue_box_in_black_plate)
    formula_idx = 3: phi_3 = hand_in_black_plate -> F( red_box_in_blue_plate)

    formula_idx = 4: phi_4 = hand_in_black_plate -> F( blue_box_in_red_plate)

    formula_idx = 5: phi_5 = phi_1 ^ phi_2

    formula_idx = 6: phi_6 = phi_1 ^ phi_2 ^ phi_3

    formula_idx = 7: phi_7 = phi_1 ^ phi_2  ^ phi_3 ^ phi_4
    
    returns: next_astate, reward, done, robustness of disjunction of outgoing predicates at (s,astate) (D^astate_phi)
 '''" code nil nil [21 838])
            ("config_bak" include nil nil [900 937])
            ("Q" variable nil nil [939 986])
            ("s" variable nil nil [987 1010])
            ("q" variable nil nil [1011 1016])
            ("formula_idx" variable nil nil [1017 1032])
            ("qn1, r1, done1, Q1, Dq1" code nil nil [1035 1106])
            ("print" code nil nil [1108 1136])
            ("rlfps.rewards.TL.automata_guided_reward.fsa_reward" include nil nil [1180 1252])
            ("config_bak" include nil nil [1253 1290])
            ("external_libs.lomap.classes" include nil nil [1291 1334])
            ("rlfps.automata_utils.plot_dynamic_graph" include nil nil [1335 1406])
            ("h" variable nil nil [1408 1479])
            ("b" variable nil nil [1480 1549])
            ("hbl" variable nil nil [1550 1624])
            ("reward_dict" variable nil nil [1626 2025])
            ("spec" variable nil nil [2027 2072])
            ("aut" variable nil nil [2073 2084])
            ("aut" code nil nil [2085 2107])
            ("aut" code nil nil [2108 2128])
            ("aut" code nil nil [2129 2156])
            ("aut_plot" variable nil nil [2158 2193])
            ("fsa_reward" variable nil nil [2195 2235])
            ("a" variable nil nil [2237 2242])
            ("sp" variable nil nil [2243 2249])
            ("qn2, r2, edge, done2, Dq2" code nil nil [2251 2331])
            ("print" code nil nil [2333 2360])
            ("print" code nil nil [2361 2389])
            ("aut_plot" code nil nil [2533 2559])
            ("input_list" variable nil nil [3100 3118]))          
      :file "test.py"
      :pointmax 3170
      :fsize 3169
      :lastmodtime '(23044 54161 628709 598000)
      :unmatched-syntax '((NAME 3119 . 3129) ($EOI 3170 . 3170)))
    (semanticdb-table "config_bak.py"
      :file "config_bak.py"
      :fsize 24380
      :lastmodtime '(23044 54161 556710 149000)))
  :file "!home!burobotics!baxter_ws!src!fsa_guided_rl!src!fsa_guided_rl!additional_libs!iclr-2018!baxter_example!simple_example!semantic.cache"
  :semantic-tag-version "2.0"
  :semanticdb-version "2.2")
