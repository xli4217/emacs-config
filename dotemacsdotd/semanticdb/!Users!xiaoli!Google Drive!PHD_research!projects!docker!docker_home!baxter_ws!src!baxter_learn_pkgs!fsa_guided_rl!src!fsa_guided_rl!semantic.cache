;; Object fsa_guided_rl/
;; SEMANTICDB Tags save file
(semanticdb-project-database-file "fsa_guided_rl/"
  :tables
  (list
    (semanticdb-table "switch_policy.py"
      :major-mode 'python-mode
      :tags 
        '( ("numpy" include nil nil [24 42])
            ("tensorflow" include nil nil [43 66])
            ("os" include nil nil [67 76])
            ("mlp_policy" include nil nil [77 109])
            ("deploy_model" include nil nil [110 146])
            ("cloudpickle" include nil nil [147 165])
            ("SwitchPolicy" type
               (:members 
                  ( ("__init__" function
                       (:suite 
                          ( ("'''
        config_list contains the configuration necessary to initialize trained policies and perform robustness comparison
        config contains the fsa for the current task (the product FSA)
        '''" code nil (reparse-symbol indented_block_body) [244 452])
                            ("self" variable nil (reparse-symbol indented_block_body) [461 482])
                            ("self" variable nil (reparse-symbol indented_block_body) [491 509])
                            ("self" variable nil (reparse-symbol indented_block_body) [518 544])
                            ("for" code nil (reparse-symbol indented_block_body) [553 818]))                          
                        :parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [208 212])
                            ("config" variable nil (reparse-symbol function_parameters) [214 220])
                            ("config_list" variable nil (reparse-symbol function_parameters) [222 233]))                          
                        :constructor-flag t)
                        (reparse-symbol indented_block_body) [195 818])
                    ("get_action" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [850 854])
                            ("obs" variable nil (reparse-symbol function_parameters) [856 859]))                          )
                        (reparse-symbol indented_block_body) [835 2549]))                  
                :type "class")
                nil [168 2549])
            ("if" code nil nil [2566 3142]))          
      :file "switch_policy.py"
      :pointmax 3146
      :fsize 3145
      :lastmodtime '(23119 62575 0 0)
      :unmatched-syntax nil)
    (semanticdb-table "mlp_policy.py"
      :file "mlp_policy.py"
      :fsize 10422
      :lastmodtime '(23119 62575 0 0))
    (semanticdb-table "deploy_model.py"
      :file "deploy_model.py"
      :fsize 1153
      :lastmodtime '(23119 62574 0 0))
    (semanticdb-table "ppo.py"
      :major-mode 'python-mode
      :tags 
        '( ("\"\"\"
PPO: Proximal Policy Optimization
Written by Patrick Coady (pat-coady.github.io)
PPO uses a loss function and gradient descent to approximate
Trust Region Policy Optimization (TRPO). See these papers for
details:

TRPO / PPO:
https://arxiv.org/pdf/1502.05477.pdf (Schulman et al., 2016)

Distributed PPO:
https://arxiv.org/abs/1707.02286 (Heess et al., 2017)

Generalized Advantage Estimation:
https://arxiv.org/pdf/1506.02438.pdf

And, also, this GitHub repo which was helpful to me during
implementation:
https://github.com/joschu/modular_rl

\"\"\"" code nil nil [1 553])
            ("gym" include nil nil [555 565])
            ("gym" include nil nil [566 590])
            ("mlp_policy" include nil nil [591 623])
            ("mlp_value" include nil nil [624 654])
            ("scipy.signal" include nil nil [655 674])
            ("utils" include nil nil [675 707])
            ("datetime" include nil nil [708 737])
            ("os" include nil nil [738 747])
            ("signal" include nil nil [748 761])
            ("config" include nil nil [762 787])
            ("rlfps.misc.logger" include nil nil [788 822])
            ("numpy" include nil nil [823 841])
            ("tensorflow" include nil nil [842 865])
            ("logger" code nil nil [868 939])
            ("GracefulKiller" type
               (:documentation " Gracefully exit program on CTRL-C "
                :members 
                  ( ("__init__" function
                       (:suite 
                          ( ("self" variable nil (reparse-symbol indented_block_body) [1041 1062])
                            ("signal" code nil (reparse-symbol indented_block_body) [1071 1121])
                            ("signal" code nil (reparse-symbol indented_block_body) [1130 1181]))                          
                        :parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [1026 1030]))                          
                        :constructor-flag t)
                        (reparse-symbol indented_block_body) [1013 1182])
                    ("exit_gracefully" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [1207 1211])
                            ("signum" variable nil (reparse-symbol function_parameters) [1213 1219])
                            ("frame" variable nil (reparse-symbol function_parameters) [1221 1226]))                          )
                        (reparse-symbol indented_block_body) [1187 1258]))                  
                :type "class")
                nil [941 1258])
            ("init_gym" function (:documentation "
    Initialize gym environment, return dimension of observation
    and action spaces.
    Args:
    env_name: str environment name (e.g. \"Humanoid-v1\")
    Returns: 3-tuple
    gym environment (object)
    number of observation dimensions (int)
    number of action dimensions (int)
    ") nil [1260 1729])
            ("run_episode" function
               (:documentation " Run single episode with option to animate
    Args:
    env: ai gym environment
    policy: policy object with sample() method
    scaler: scaler object, used to scale/offset each observation dimension
    to a similar range
    animate: boolean, True uses env.render() method to animate episode
    
    Returns: 4-tuple of NumPy arrays
    observes: shape = (episode len, obs_dim)
    actions: shape = (episode len, act_dim)
    rewards: shape = (episode len,)
    unscaled_obs: useful for training scaler, shape = (episode len, obs_dim)
    "
                :arguments 
                  ( ("env" variable nil (reparse-symbol function_parameters) [1747 1750])
                    ("policy" variable nil (reparse-symbol function_parameters) [1752 1758])
                    ("scaler" variable nil (reparse-symbol function_parameters) [1760 1766])
                    ("animate" variable nil (reparse-symbol function_parameters) [1768 1775]))                  )
                nil [1731 3245])
            ("run_policy" function
               (:documentation " Run policy and collect data for a minimum of min_steps and min_episodes
    Args:
             env: a gym environment
             policy: policy object with sample() method
             scaler: scaler object, used to scale/offset each observation dimension
                 to a similar range
             episodes: total episodes to run
         Returns: list of trajectory dictionaries, list length = number of episodes
             'observes' : NumPy array of states from episode
             'actions' : NumPy array of actions from episode
             'rewards' : NumPy array of (un-discounted) rewards from episode
             'unscaled_obs' : NumPy array of (un-discounted) rewards from episode
     "
                :arguments 
                  ( ("env" variable nil (reparse-symbol function_parameters) [3261 3264])
                    ("policy" variable nil (reparse-symbol function_parameters) [3266 3272])
                    ("scaler" variable nil (reparse-symbol function_parameters) [3274 3280])
                    ("batch_size" variable nil (reparse-symbol function_parameters) [3282 3292]))                  )
                nil [3246 5463])
            ("discount" function
               (:documentation " Calculate discounted forward sum of a sequence at each point "
                :arguments 
                  ( ("x" variable nil (reparse-symbol function_parameters) [5477 5478])
                    ("gamma" variable nil (reparse-symbol function_parameters) [5480 5485]))                  )
                nil [5464 5630])
            ("add_disc_sum_rew" function
               (:documentation " Adds discounted sum of rewards to all time steps of all trajectories
    Args:
    trajectories: as returned by run_policy()
    gamma: discount

    Returns:
    None (mutates trajectories dictionary to add 'disc_sum_rew')
    "
                :arguments 
                  ( ("trajectories" variable nil (reparse-symbol function_parameters) [5653 5665])
                    ("gamma" variable nil (reparse-symbol function_parameters) [5667 5672]))                  )
                nil [5632 6221])
            ("add_value" function
               (:documentation " Adds estimated value to all time steps of all trajectories
    Args:
    trajectories: as returned by run_policy()
    val_func: object with predict() method, takes observations
    and returns predicted state value
    Returns:
    None (mutates trajectories dictionary to add 'values')
    "
                :arguments 
                  ( ("trajectories" variable nil (reparse-symbol function_parameters) [6237 6249])
                    ("val_func" variable nil (reparse-symbol function_parameters) [6251 6259]))                  )
                nil [6223 6726])
            ("add_gae" function
               (:documentation " Add generalized advantage estimator.
    https://arxiv.org/pdf/1506.02438.pdf
    Args:
    trajectories: as returned by run_policy(), must include 'values'
    key from add_value().
    gamma: reward discount
    lam: lambda (see paper).
    lam=0 : use TD residuals
    lam=1 : A =  Sum Discounted Rewards - V_hat(s)

    Returns:
    None (mutates trajectories dictionary to add 'advantages')
    "
                :arguments 
                  ( ("trajectories" variable nil (reparse-symbol function_parameters) [6740 6752])
                    ("gamma" variable nil (reparse-symbol function_parameters) [6754 6759])
                    ("lam" variable nil (reparse-symbol function_parameters) [6761 6764]))                  )
                nil [6728 7616])
            ("build_train_set" function
               (:documentation "
    Args:
    trajectories: trajectories after processing by add_disc_sum_rew(),
    add_value(), and add_gae()

    Returns: 4-tuple of NumPy arrays
    observes: shape = (N, obs_dim)
    actions: shape = (N, act_dim)
    advantages: shape = (N,)
    disc_sum_rew: shape = (N,)
    "
                :arguments 
                  ( ("trajectories" variable nil (reparse-symbol function_parameters) [7638 7650]))                  )
                nil [7618 8400])
            ("log_batch_stats" function
               (:documentation " Log various batch statistics "
                :arguments 
                  ( ("observes" variable nil (reparse-symbol function_parameters) [8422 8430])
                    ("actions" variable nil (reparse-symbol function_parameters) [8432 8439])
                    ("advantages" variable nil (reparse-symbol function_parameters) [8441 8451])
                    ("disc_sum_rew" variable nil (reparse-symbol function_parameters) [8453 8465])
                    ("iteration" variable nil (reparse-symbol function_parameters) [8467 8476]))                  )
                nil [8402 9574])
            ("train" function
               (:documentation " Main training loop
    Args:
    env_name: OpenAI Gym environment name, e.g. 'Hopper-v1'
    num_episodes: maximum number of episodes to run
    gamma: reward discount factor (float)
    lam: lambda from Generalized Advantage Estimate
    kl_targ: D_KL target for policy update [D_KL(pi_old || pi_new)
    batch_size: number of episodes per policy training batch
    "
                :arguments 
                  ( ("max_iterations" variable nil (reparse-symbol function_parameters) [9590 9604])
                    ("gamma" variable nil (reparse-symbol function_parameters) [9606 9611])
                    ("lam" variable nil (reparse-symbol function_parameters) [9613 9616])
                    ("batch_size" variable nil (reparse-symbol function_parameters) [9618 9628]))                  )
                nil [9580 11721])
            ("if" code nil nil [11723 12176]))          
      :file "ppo.py"
      :pointmax 12180
      :fsize 12179
      :lastmodtime '(23119 62575 0 0)
      :unmatched-syntax nil)
    (semanticdb-table "mlp_value.py"
      :file "mlp_value.py"
      :fsize 4739
      :lastmodtime '(23119 62575 0 0))
    (semanticdb-table "utils.py"
      :file "utils.py"
      :fsize 4103
      :lastmodtime '(23119 62575 0 0))
    (semanticdb-table "config.py"
      :major-mode 'python-mode
      :tags 
        '( ("os" include nil nil [1 10])
            ("numpy" include nil nil [11 29])
            ("get_rewards" include nil nil [30 81])
            ("formula_idx" variable nil nil [171 186])
            ("if" code nil nil [206 290])
            ("EXPERIMENT_NAME" code nil nil [291 334])
            ("file_path" variable nil nil [335 373])
            ("file_dir" variable nil nil [374 396])
            ("experiment_dir" variable nil nil [397 468])
            ("spaces, rewards" code nil nil [794 883])
            ("CONFIG" variable nil nil [885 2426]))          
      :file "config.py"
      :pointmax 2648
      :fsize 2647
      :lastmodtime '(23119 62574 0 0)
      :unmatched-syntax nil)
    (semanticdb-table "baxter_env.py"
      :major-mode 'python-mode
      :tags 
        '( ("rospy" include nil nil [24 36])
            ("os" include nil nil [37 46])
            ("numpy" include nil nil [47 65])
            ("baxter_api.baxter_utils" include nil nil [66 111])
            ("baxter_api.perception.optitrack.get_env_info" include nil nil [112 181])
            ("rlfps.automata_utils.plot_dynamic_graph" include nil nil [182 253])
            ("BaxterEnv" type
               (:members 
                  ( ("__init__" function
                       (:suite 
                          ( ("self" variable nil (reparse-symbol indented_block_body) [314 343])
                            ("self" variable nil (reparse-symbol indented_block_body) [352 377])
                            ("self" variable nil (reparse-symbol indented_block_body) [395 449])
                            ("self" variable nil (reparse-symbol indented_block_body) [458 512])
                            ("self" variable nil (reparse-symbol indented_block_body) [522 578])
                            ("self" variable nil (reparse-symbol indented_block_body) [587 643])
                            ("self" variable nil (reparse-symbol indented_block_body) [653 691])
                            ("if" code nil (reparse-symbol indented_block_body) [700 1075])
                            ("self" variable nil (reparse-symbol indented_block_body) [1083 1103]))                          
                        :parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [291 295])
                            ("config" variable nil (reparse-symbol function_parameters) [297 303]))                          
                        :constructor-flag t)
                        (reparse-symbol indented_block_body) [278 1104])
                    ("_reset" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [1128 1132]))                          )
                        (reparse-symbol indented_block_body) [1117 1907])
                    ("get_current_state" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [1934 1938])
                            ("policy_idx" variable nil (reparse-symbol function_parameters) [1940 1950]))                          )
                        (reparse-symbol indented_block_body) [1912 3251])
                    ("_step" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [3278 3282])
                            ("u" variable nil (reparse-symbol function_parameters) [3284 3285])
                            ("policy_idx" variable nil (reparse-symbol function_parameters) [3287 3297]))                          )
                        (reparse-symbol indented_block_body) [3268 4215])
                    ("get_switched_qstates" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [4262 4266])
                            ("states" variable nil (reparse-symbol function_parameters) [4268 4274]))                          )
                        (reparse-symbol indented_block_body) [4237 5704]))                  
                :type "class")
                nil [255 5704])
            ("if" code nil nil [5714 5867]))          
      :file "baxter_env.py"
      :pointmax 5867
      :fsize 5866
      :lastmodtime '(23119 62574 0 0)
      :unmatched-syntax nil)
    (semanticdb-table "get_rewards.py"
      :major-mode 'python-mode
      :tags 
        '( ("os" include nil nil [1 10])
            ("numpy" include nil nil [11 29])
            ("rlfps.rewards.TL.automata_guided_reward.fsa_reward" include nil nil [30 102])
            ("lomap.classes" include nil nil [103 132])
            ("rlfps.automata_utils.plot_dynamic_graph" include nil nil [133 204])
            ("fsa_guided_rl" include nil nil [206 226])
            ("baxter_api.env.config" include nil nil [293 350])
            ("baxter_api.env.config" include nil nil [351 397])
            ("table_origin" variable nil nil [469 500])
            ("table_width" variable nil nil [554 572])
            ("table_length" variable nil nil [573 592])
            ("table_thickness" variable nil nil [593 612])
            ("table_bound_shrink" variable nil nil [614 638])
            ("table_planar_ub" variable nil nil [639 781])
            ("table_planar_lb" variable nil nil [782 924])
            ("red_button_xy" variable nil nil [1021 1093])
            ("red_button_r" code nil nil [1094 1167])
            ("blue_button_xy" variable nil nil [1169 1243])
            ("blue_button_r" code nil nil [1244 1319])
            ("mode" variable nil nil [1411 1433])
            ("if" code nil nil [1434 1498])
            ("red_plate_xy" variable nil nil [1503 1573])
            ("red_plate_width" code nil nil [1574 1656])
            ("red_plate_length" code nil nil [1667 1750])
            ("blue_plate_xy" variable nil nil [1762 1834])
            ("blue_plate_width" code nil nil [1835 1919])
            ("blue_plate_length" code nil nil [1930 2015])
            ("black_plate_xy" variable nil nil [2027 2101])
            ("black_plate_width" code nil nil [2102 2188])
            ("black_plate_length" code nil nil [2199 2285])
            ("hand_in_red" function (:arguments 
              ( ("s" variable nil (reparse-symbol function_parameters) [2313 2314]))              ) nil [2297 2464])
            ("hand_in_blue" function (:arguments 
              ( ("s" variable nil (reparse-symbol function_parameters) [2482 2483]))              ) nil [2465 2636])
            ("hand_in_black" function (:arguments 
              ( ("s" variable nil (reparse-symbol function_parameters) [2655 2656]))              ) nil [2637 3024])
            ("get_robustness" function (:arguments 
              ( ("s" variable nil (reparse-symbol function_parameters) [3080 3081])
                ("s_type" variable nil (reparse-symbol function_parameters) [3082 3088])
                ("instruction" variable nil (reparse-symbol function_parameters) [3089 3100]))              ) nil [3061 7929])
            ("TL2_robustness_func" function (:arguments 
              ( ("traj" variable nil (reparse-symbol function_parameters) [7980 7984])
                ("formula_idx" variable nil (reparse-symbol function_parameters) [7986 7997]))              ) nil [7956 11859])
            ("heuristic_reward2" function (:arguments 
              ( ("s" variable nil (reparse-symbol function_parameters) [11890 11891])
                ("hr" variable nil (reparse-symbol function_parameters) [11893 11895])
                ("hb" variable nil (reparse-symbol function_parameters) [11897 11899])
                ("hbl" variable nil (reparse-symbol function_parameters) [11901 11904])
                ("formula_idx" variable nil (reparse-symbol function_parameters) [11906 11917]))              ) nil [11868 14396])
            ("reward_dict" variable nil nil [14411 15324])
            ("'''
s = [(x,y)_hand, (x,y)_red_box]
astate takes 0,1,2,3

formula_idx = 1: phi_1 = (hand_in_red_button -> F red_box_in_red_plate) ^  (!hand_in_red_button -> F red_box_in_black_plate)

formula_idx = 2: phi_2 = (hand_in_red_button -> F red_box_in_red_plate) ^  (!(hand_in_red_button v hand_in_black_plate) -> F red_box_in_black_plate)

formula_idx = 3: phi_3 = (hand_in_blue_button -> F blue_box_in_blue_plate) ^  (!(hand_in_blue_button v hand_in_black_plate) -> F blue_box_in_black_plate)

formula_idx = 4: phi_4 = hand_in_black_plate -> F( red_box_in_blue_plate)

formula_idx = 5: phi_5 = hand_in_black_plate -> F( blue_box_in_red_plate)

formula_idx = 6: phi_6 = phi_2 ^ phi_3

formula_idx = 7: phi_7 = phi_2 ^ phi_3  ^ phi_4 ^ phi_5
    
'''" code nil nil [15327 16070])
            ("spec1" variable nil nil [16072 16114])
            ("spec2" variable nil nil [16115 16166])
            ("spec3" variable nil nil [16167 16218])
            ("spec4" variable nil nil [16219 16243])
            ("spec5" variable nil nil [16244 16268])
            ("spec6" code nil nil [16269 16299])
            ("spec7" code nil nil [16300 16364])
            ("specs" variable nil nil [16366 16417])
            ("get_reward_and_spaces" function (:arguments 
              ( ("formula_idx" variable nil (reparse-symbol function_parameters) [16448 16459])
                ("reward_type" variable nil (reparse-symbol function_parameters) [16461 16472]))              ) nil [16422 23677]))          
      :file "get_rewards.py"
      :pointmax 23682
      :fsize 23681
      :lastmodtime '(23119 62575 0 0)
      :unmatched-syntax nil))
  :file "!Users!xiaoli!Google Drive!PHD_research!projects!docker!docker_home!baxter_ws!src!baxter_learn_pkgs!fsa_guided_rl!src!fsa_guided_rl!semantic.cache"
  :semantic-tag-version "2.0"
  :semanticdb-version "2.2")
