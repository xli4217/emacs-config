;; Object semanticdb-project-database-file
;; SEMANTICDB Tags save file
(semanticdb-project-database-file "semanticdb-project-database-file"
  :tables
  (list
    (semanticdb-table "semanticdb-table"
      :major-mode python-mode
      :tags 
        '( ("copy" include nil nil [1 26])
            ("itertools" include nil nil [27 43])
            ("numpy" include nil nil [44 62])
            ("torch" include nil nil [63 75])
            ("torch.optim" include nil nil [76 104])
            ("gym" include nil nil [105 115])
            ("time" include nil nil [116 127])
            ("spinup.algos.pytorch.td3.core" include nil nil [128 172])
            ("spinup.utils.logx" include nil nil [173 214])
            ("ReplayBuffer" type
               (:documentation "
    A simple FIFO experience replay buffer for TD3 agents.
    "
                :members 
                  ( ("__init__" function
                       (:suite 
                          ( ("self" variable nil (reparse-symbol indented_block_body) [369 446])
                            ("self" variable nil (reparse-symbol indented_block_body) [455 533])
                            ("self" variable nil (reparse-symbol indented_block_body) [542 619])
                            ("self" variable nil (reparse-symbol indented_block_body) [628 675])
                            ("self" variable nil (reparse-symbol indented_block_body) [684 732])
                            ("self, self, self" code nil (reparse-symbol indented_block_body) [741 788]))                          
                        :parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [330 334])
                            ("obs_dim" variable nil (reparse-symbol function_parameters) [336 343])
                            ("act_dim" variable nil (reparse-symbol function_parameters) [345 352])
                            ("size" variable nil (reparse-symbol function_parameters) [354 358]))                          
                        :constructor-flag t)
                        (reparse-symbol indented_block_body) [317 789])
                    ("store" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [804 808])
                            ("obs" variable nil (reparse-symbol function_parameters) [810 813])
                            ("act" variable nil (reparse-symbol function_parameters) [815 818])
                            ("rew" variable nil (reparse-symbol function_parameters) [820 823])
                            ("next_obs" variable nil (reparse-symbol function_parameters) [825 833])
                            ("done" variable nil (reparse-symbol function_parameters) [835 839]))                          )
                        (reparse-symbol indented_block_body) [794 1135])
                    ("sample_batch" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [1157 1161])
                            ("batch_size" variable nil (reparse-symbol function_parameters) [1163 1173]))                          )
                        (reparse-symbol indented_block_body) [1140 1557]))                  
                :type "class")
                nil [217 1557])
            ("td3" function
               (:documentation "
    Twin Delayed Deep Deterministic Policy Gradient (TD3)


    Args:
        env_fn : A function which creates a copy of the environment.
            The environment must satisfy the OpenAI Gym API.

        actor_critic: The constructor method for a PyTorch Module with an ``act`` 
            method, a ``pi`` module, a ``q1`` module, and a ``q2`` module.
            The ``act`` method and ``pi`` module should accept batches of 
            observations as inputs, and ``q1`` and ``q2`` should accept a batch 
            of observations and a batch of actions as inputs. When called, 
            these should return:

            ===========  ================  ======================================
            Call         Output Shape      Description
            ===========  ================  ======================================
            ``act``      (batch, act_dim)  | Numpy array of actions for each 
                                           | observation.
            ``pi``       (batch, act_dim)  | Tensor containing actions from policy
                                           | given observations.
            ``q1``       (batch,)          | Tensor containing one current estimate
                                           | of Q* for the provided observations
                                           | and actions. (Critical: make sure to
                                           | flatten this!)
            ``q2``       (batch,)          | Tensor containing the other current 
                                           | estimate of Q* for the provided observations
                                           | and actions. (Critical: make sure to
                                           | flatten this!)
            ===========  ================  ======================================

        ac_kwargs (dict): Any kwargs appropriate for the ActorCritic object 
            you provided to TD3.

        seed (int): Seed for random number generators.

        steps_per_epoch (int): Number of steps of interaction (state-action pairs) 
            for the agent and the environment in each epoch.

        epochs (int): Number of epochs to run and train agent.

        replay_size (int): Maximum length of replay buffer.

        gamma (float): Discount factor. (Always between 0 and 1.)

        polyak (float): Interpolation factor in polyak averaging for target 
            networks. Target networks are updated towards main networks 
            according to:

            .. math:: \\\\theta_{\\\\text{targ}} \\\\leftarrow 
                \\\\rho \\\\theta_{\\\\text{targ}} + (1-\\\\rho) \\\\theta

            where :math:`\\\\rho` is polyak. (Always between 0 and 1, usually 
            close to 1.)

        pi_lr (float): Learning rate for policy.

        q_lr (float): Learning rate for Q-networks.

        batch_size (int): Minibatch size for SGD.

        start_steps (int): Number of steps for uniform-random action selection,
            before running real policy. Helps exploration.

        update_after (int): Number of env interactions to collect before
            starting to do gradient descent updates. Ensures replay buffer
            is full enough for useful updates.

        update_every (int): Number of env interactions that should elapse
            between gradient descent updates. Note: Regardless of how long 
            you wait between updates, the ratio of env steps to gradient steps 
            is locked to 1.

        act_noise (float): Stddev for Gaussian exploration noise added to 
            policy at training time. (At test time, no noise is added.)

        target_noise (float): Stddev for smoothing noise added to target 
            policy.

        noise_clip (float): Limit for absolute value of target policy 
            smoothing noise.

        policy_delay (int): Policy will only be updated once every 
            policy_delay times for each update of the Q-networks.

        num_test_episodes (int): Number of episodes to test the deterministic
            policy at the end of each epoch.

        max_ep_len (int): Maximum length of trajectory / episode / rollout.

        logger_kwargs (dict): Keyword args for EpochLogger.

        save_freq (int): How often (in terms of gap between epochs) to save
            the current policy and value function.

    "
                :arguments 
                  ( ("env_fn" variable nil (reparse-symbol function_parameters) [1568 1574])
                    ("actor_critic" variable nil (reparse-symbol function_parameters) [1576 1588])
                    ("ac_kwargs" variable nil (reparse-symbol function_parameters) [1610 1619])
                    ("seed" variable nil (reparse-symbol function_parameters) [1628 1632])
                    ("steps_per_epoch" variable nil (reparse-symbol function_parameters) [1645 1660])
                    ("epochs" variable nil (reparse-symbol function_parameters) [1667 1673])
                    ("replay_size" variable nil (reparse-symbol function_parameters) [1679 1690])
                    ("gamma" variable nil (reparse-symbol function_parameters) [1701 1706])
                    ("polyak" variable nil (reparse-symbol function_parameters) [1722 1728])
                    ("pi_lr" variable nil (reparse-symbol function_parameters) [1736 1741])
                    ("q_lr" variable nil (reparse-symbol function_parameters) [1748 1752])
                    ("batch_size" variable nil (reparse-symbol function_parameters) [1759 1769])
                    ("start_steps" variable nil (reparse-symbol function_parameters) [1775 1786])
                    ("update_after" variable nil (reparse-symbol function_parameters) [1803 1815])
                    ("update_every" variable nil (reparse-symbol function_parameters) [1822 1834])
                    ("act_noise" variable nil (reparse-symbol function_parameters) [1839 1848])
                    ("target_noise" variable nil (reparse-symbol function_parameters) [1854 1866])
                    ("noise_clip" variable nil (reparse-symbol function_parameters) [1881 1891])
                    ("policy_delay" variable nil (reparse-symbol function_parameters) [1897 1909])
                    ("num_test_episodes" variable nil (reparse-symbol function_parameters) [1913 1930])
                    ("max_ep_len" variable nil (reparse-symbol function_parameters) [1935 1945])
                    ("logger_kwargs" variable nil (reparse-symbol function_parameters) [1961 1974])
                    ("save_freq" variable nil (reparse-symbol function_parameters) [1983 1992]))                  )
                nil [1560 13932])
            ("if" code nil nil [13933 14824]))          
      :file "td3.py"
      :pointmax 14824
      :fsize 14823
      :lastmodtime '(24579 51013 801489 660000)
      :unmatched-syntax '((NAME 12171 . 12172) (IF 12181 . 12183) (ELSE 12203 . 12207)))
    (semanticdb-table "semanticdb-table"
      :file "core.py"
      :fsize 2069
      :lastmodtime '(24579 51013 777489 551000)))
  :file "!home!xli4217!Dropbox!docker!docker_home!lof_robot_learning!libs!spinningup!spinup!algos!pytorch!td3!semantic.cache"
  :semantic-tag-version "2.0"
  :semanticdb-version "2.2")
