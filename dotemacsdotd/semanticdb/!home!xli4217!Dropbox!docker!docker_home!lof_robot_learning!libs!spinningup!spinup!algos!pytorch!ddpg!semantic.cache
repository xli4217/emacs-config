;; Object semanticdb-project-database-file
;; SEMANTICDB Tags save file
(semanticdb-project-database-file "semanticdb-project-database-file"
  :tables
  (list
    (semanticdb-table "semanticdb-table"
      :major-mode python-mode
      :tags 
        '( ("copy" include nil nil [1 26])
            ("numpy" include nil nil [27 45])
            ("torch" include nil nil [46 58])
            ("torch.optim" include nil nil [59 87])
            ("gym" include nil nil [88 98])
            ("time" include nil nil [99 110])
            ("spinup.algos.pytorch.ddpg.core" include nil nil [111 156])
            ("spinup.utils.logx" include nil nil [157 198])
            ("ReplayBuffer" type
               (:documentation "
    A simple FIFO experience replay buffer for DDPG agents.
    "
                :members 
                  ( ("__init__" function
                       (:suite 
                          ( ("self" variable nil (reparse-symbol indented_block_body) [354 431])
                            ("self" variable nil (reparse-symbol indented_block_body) [440 518])
                            ("self" variable nil (reparse-symbol indented_block_body) [527 604])
                            ("self" variable nil (reparse-symbol indented_block_body) [613 660])
                            ("self" variable nil (reparse-symbol indented_block_body) [669 717])
                            ("self, self, self" code nil (reparse-symbol indented_block_body) [726 773]))                          
                        :parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [315 319])
                            ("obs_dim" variable nil (reparse-symbol function_parameters) [321 328])
                            ("act_dim" variable nil (reparse-symbol function_parameters) [330 337])
                            ("size" variable nil (reparse-symbol function_parameters) [339 343]))                          
                        :constructor-flag t)
                        (reparse-symbol indented_block_body) [302 774])
                    ("store" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [789 793])
                            ("obs" variable nil (reparse-symbol function_parameters) [795 798])
                            ("act" variable nil (reparse-symbol function_parameters) [800 803])
                            ("rew" variable nil (reparse-symbol function_parameters) [805 808])
                            ("next_obs" variable nil (reparse-symbol function_parameters) [810 818])
                            ("done" variable nil (reparse-symbol function_parameters) [820 824]))                          )
                        (reparse-symbol indented_block_body) [779 1120])
                    ("sample_batch" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [1142 1146])
                            ("batch_size" variable nil (reparse-symbol function_parameters) [1148 1158]))                          )
                        (reparse-symbol indented_block_body) [1125 1542]))                  
                :type "class")
                nil [201 1542])
            ("ddpg" function
               (:documentation "
    Deep Deterministic Policy Gradient (DDPG)


    Args:
        env_fn : A function which creates a copy of the environment.
            The environment must satisfy the OpenAI Gym API.

        actor_critic: The constructor method for a PyTorch Module with an ``act`` 
            method, a ``pi`` module, and a ``q`` module. The ``act`` method and
            ``pi`` module should accept batches of observations as inputs,
            and ``q`` should accept a batch of observations and a batch of 
            actions as inputs. When called, these should return:

            ===========  ================  ======================================
            Call         Output Shape      Description
            ===========  ================  ======================================
            ``act``      (batch, act_dim)  | Numpy array of actions for each 
                                           | observation.
            ``pi``       (batch, act_dim)  | Tensor containing actions from policy
                                           | given observations.
            ``q``        (batch,)          | Tensor containing the current estimate
                                           | of Q* for the provided observations
                                           | and actions. (Critical: make sure to
                                           | flatten this!)
            ===========  ================  ======================================

        ac_kwargs (dict): Any kwargs appropriate for the ActorCritic object 
            you provided to DDPG.

        seed (int): Seed for random number generators.

        steps_per_epoch (int): Number of steps of interaction (state-action pairs) 
            for the agent and the environment in each epoch.

        epochs (int): Number of epochs to run and train agent.

        replay_size (int): Maximum length of replay buffer.

        gamma (float): Discount factor. (Always between 0 and 1.)

        polyak (float): Interpolation factor in polyak averaging for target 
            networks. Target networks are updated towards main networks 
            according to:

            .. math:: \\\\theta_{\\\\text{targ}} \\\\leftarrow 
                \\\\rho \\\\theta_{\\\\text{targ}} + (1-\\\\rho) \\\\theta

            where :math:`\\\\rho` is polyak. (Always between 0 and 1, usually 
            close to 1.)

        pi_lr (float): Learning rate for policy.

        q_lr (float): Learning rate for Q-networks.

        batch_size (int): Minibatch size for SGD.

        start_steps (int): Number of steps for uniform-random action selection,
            before running real policy. Helps exploration.

        update_after (int): Number of env interactions to collect before
            starting to do gradient descent updates. Ensures replay buffer
            is full enough for useful updates.

        update_every (int): Number of env interactions that should elapse
            between gradient descent updates. Note: Regardless of how long 
            you wait between updates, the ratio of env steps to gradient steps 
            is locked to 1.

        act_noise (float): Stddev for Gaussian exploration noise added to 
            policy at training time. (At test time, no noise is added.)

        num_test_episodes (int): Number of episodes to test the deterministic
            policy at the end of each epoch.

        max_ep_len (int): Maximum length of trajectory / episode / rollout.

        logger_kwargs (dict): Keyword args for EpochLogger.

        save_freq (int): How often (in terms of gap between epochs) to save
            the current policy and value function.

    "
                :arguments 
                  ( ("env_fn" variable nil (reparse-symbol function_parameters) [1554 1560])
                    ("actor_critic" variable nil (reparse-symbol function_parameters) [1562 1574])
                    ("ac_kwargs" variable nil (reparse-symbol function_parameters) [1596 1605])
                    ("seed" variable nil (reparse-symbol function_parameters) [1614 1618])
                    ("steps_per_epoch" variable nil (reparse-symbol function_parameters) [1632 1647])
                    ("epochs" variable nil (reparse-symbol function_parameters) [1654 1660])
                    ("replay_size" variable nil (reparse-symbol function_parameters) [1666 1677])
                    ("gamma" variable nil (reparse-symbol function_parameters) [1688 1693])
                    ("polyak" variable nil (reparse-symbol function_parameters) [1710 1716])
                    ("pi_lr" variable nil (reparse-symbol function_parameters) [1724 1729])
                    ("q_lr" variable nil (reparse-symbol function_parameters) [1736 1740])
                    ("batch_size" variable nil (reparse-symbol function_parameters) [1747 1757])
                    ("start_steps" variable nil (reparse-symbol function_parameters) [1763 1774])
                    ("update_after" variable nil (reparse-symbol function_parameters) [1792 1804])
                    ("update_every" variable nil (reparse-symbol function_parameters) [1811 1823])
                    ("act_noise" variable nil (reparse-symbol function_parameters) [1828 1837])
                    ("num_test_episodes" variable nil (reparse-symbol function_parameters) [1843 1860])
                    ("max_ep_len" variable nil (reparse-symbol function_parameters) [1875 1885])
                    ("logger_kwargs" variable nil (reparse-symbol function_parameters) [1892 1905])
                    ("save_freq" variable nil (reparse-symbol function_parameters) [1914 1923]))                  )
                nil [1545 12120])
            ("if" code nil nil [12121 13017]))          
      :file "ddpg.py"
      :pointmax 13017
      :fsize 13016
      :lastmodtime '(24579 51013 169486 767000)
      :unmatched-syntax '((NAME 10433 . 10434) (IF 10443 . 10445) (ELSE 10465 . 10469)))
    (semanticdb-table "semanticdb-table"
      :file "core.py"
      :fsize 1993
      :lastmodtime '(24579 51013 109486 493000)))
  :file "!home!xli4217!Dropbox!docker!docker_home!lof_robot_learning!libs!spinningup!spinup!algos!pytorch!ddpg!semantic.cache"
  :semantic-tag-version "2.0"
  :semanticdb-version "2.2")
