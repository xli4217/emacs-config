;; Object src/
;; SEMANTICDB Tags save file
(semanticdb-project-database-file "src/"
  :tables
  (list
    (semanticdb-table "train.py"
      :major-mode 'python-mode
      :tags 
        '( ("\"\"\"
PPO: Proximal Policy Optimization

Written by Patrick Coady (pat-coady.github.io)

PPO uses a loss function and gradient descent to approximate
Trust Region Policy Optimization (TRPO). See these papers for
details:

TRPO / PPO:
https://arxiv.org/pdf/1502.05477.pdf (Schulman et al., 2016)

Distributed PPO:
https://arxiv.org/abs/1707.02286 (Heess et al., 2017)

Generalized Advantage Estimation:
https://arxiv.org/pdf/1506.02438.pdf

And, also, this GitHub repo which was helpful to me during
implementation:
https://github.com/joschu/modular_rl

This implementation learns policies for continuous environments
in the OpenAI Gym (https://gym.openai.com/). Testing was focused on
the MuJoCo control tasks.
\"\"\"" code nil nil [25 737])
            ("gym" include nil nil [738 748])
            ("numpy" include nil nil [749 767])
            ("gym" include nil nil [768 792])
            ("policy" include nil nil [793 818])
            ("value_function" include nil nil [819 861])
            ("scipy.signal" include nil nil [862 881])
            ("utils" include nil nil [882 914])
            ("datetime" include nil nil [915 944])
            ("os" include nil nil [945 954])
            ("argparse" include nil nil [955 970])
            ("signal" include nil nil [971 984])
            ("GracefulKiller" type
               (:documentation " Gracefully exit program on CTRL-C "
                :members 
                  ( ("__init__" function
                       (:suite 
                          ( ("self" variable nil (reparse-symbol indented_block_body) [1087 1108])
                            ("signal" code nil (reparse-symbol indented_block_body) [1117 1167])
                            ("signal" code nil (reparse-symbol indented_block_body) [1176 1227]))                          
                        :parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [1072 1076]))                          
                        :constructor-flag t)
                        (reparse-symbol indented_block_body) [1059 1228])
                    ("exit_gracefully" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [1253 1257])
                            ("signum" variable nil (reparse-symbol function_parameters) [1259 1265])
                            ("frame" variable nil (reparse-symbol function_parameters) [1267 1272]))                          )
                        (reparse-symbol indented_block_body) [1233 1304]))                  
                :type "class")
                nil [987 1304])
            ("init_gym" function
               (:documentation "
    Initialize gym environment, return dimension of observation
    and action spaces.

    Args:
        env_name: str environment name (e.g. \"Humanoid-v1\")

    Returns: 3-tuple
        gym environment (object)
        number of observation dimensions (int)
        number of action dimensions (int)
    "
                :arguments 
                  ( ("env_name" variable nil (reparse-symbol function_parameters) [1319 1327]))                  )
                nil [1306 1796])
            ("run_episode" function
               (:documentation " Run single episode with option to animate

    Args:
        env: ai gym environment
        policy: policy object with sample() method
        scaler: scaler object, used to scale/offset each observation dimension
            to a similar range
        animate: boolean, True uses env.render() method to animate episode

    Returns: 4-tuple of NumPy arrays
        observes: shape = (episode len, obs_dim)
        actions: shape = (episode len, act_dim)
        rewards: shape = (episode len,)
        unscaled_obs: useful for training scaler, shape = (episode len, obs_dim)
    "
                :arguments 
                  ( ("env" variable nil (reparse-symbol function_parameters) [1814 1817])
                    ("policy" variable nil (reparse-symbol function_parameters) [1819 1825])
                    ("scaler" variable nil (reparse-symbol function_parameters) [1827 1833])
                    ("animate" variable nil (reparse-symbol function_parameters) [1835 1842]))                  )
                nil [1798 3504])
            ("run_policy" function
               (:documentation " Run policy and collect data for a minimum of min_steps and min_episodes

    Args:
        env: ai gym environment
        policy: policy object with sample() method
        scaler: scaler object, used to scale/offset each observation dimension
            to a similar range
        logger: logger object, used to save stats from episodes
        episodes: total episodes to run

    Returns: list of trajectory dictionaries, list length = number of episodes
        'observes' : NumPy array of states from episode
        'actions' : NumPy array of actions from episode
        'rewards' : NumPy array of (un-discounted) rewards from episode
        'unscaled_obs' : NumPy array of (un-discounted) rewards from episode
    "
                :arguments 
                  ( ("env" variable nil (reparse-symbol function_parameters) [3521 3524])
                    ("policy" variable nil (reparse-symbol function_parameters) [3526 3532])
                    ("scaler" variable nil (reparse-symbol function_parameters) [3534 3540])
                    ("logger" variable nil (reparse-symbol function_parameters) [3542 3548])
                    ("episodes" variable nil (reparse-symbol function_parameters) [3550 3558]))                  )
                nil [3506 5018])
            ("discount" function
               (:documentation " Calculate discounted forward sum of a sequence at each point "
                :arguments 
                  ( ("x" variable nil (reparse-symbol function_parameters) [5033 5034])
                    ("gamma" variable nil (reparse-symbol function_parameters) [5036 5041]))                  )
                nil [5020 5186])
            ("add_disc_sum_rew" function
               (:documentation " Adds discounted sum of rewards to all time steps of all trajectories

    Args:
        trajectories: as returned by run_policy()
        gamma: discount

    Returns:
        None (mutates trajectories dictionary to add 'disc_sum_rew')
    "
                :arguments 
                  ( ("trajectories" variable nil (reparse-symbol function_parameters) [5209 5221])
                    ("gamma" variable nil (reparse-symbol function_parameters) [5223 5228]))                  )
                nil [5188 5790])
            ("add_value" function
               (:documentation " Adds estimated value to all time steps of all trajectories

    Args:
        trajectories: as returned by run_policy()
        val_func: object with predict() method, takes observations
            and returns predicted state value

    Returns:
        None (mutates trajectories dictionary to add 'values')
    "
                :arguments 
                  ( ("trajectories" variable nil (reparse-symbol function_parameters) [5806 5818])
                    ("val_func" variable nil (reparse-symbol function_parameters) [5820 5828]))                  )
                nil [5792 6317])
            ("add_gae" function
               (:documentation " Add generalized advantage estimator.
    https://arxiv.org/pdf/1506.02438.pdf

    Args:
        trajectories: as returned by run_policy(), must include 'values'
            key from add_value().
        gamma: reward discount
        lam: lambda (see paper).
            lam=0 : use TD residuals
            lam=1 : A =  Sum Discounted Rewards - V_hat(s)

    Returns:
        None (mutates trajectories dictionary to add 'advantages')
    "
                :arguments 
                  ( ("trajectories" variable nil (reparse-symbol function_parameters) [6331 6343])
                    ("gamma" variable nil (reparse-symbol function_parameters) [6345 6350])
                    ("lam" variable nil (reparse-symbol function_parameters) [6352 6355]))                  )
                nil [6319 7248])
            ("build_train_set" function
               (:documentation "

    Args:
        trajectories: trajectories after processing by add_disc_sum_rew(),
            add_value(), and add_gae()

    Returns: 4-tuple of NumPy arrays
        observes: shape = (N, obs_dim)
        actions: shape = (N, act_dim)
        advantages: shape = (N,)
        disc_sum_rew: shape = (N,)
    "
                :arguments 
                  ( ("trajectories" variable nil (reparse-symbol function_parameters) [7270 7282]))                  )
                nil [7250 8056])
            ("log_batch_stats" function
               (:documentation " Log various batch statistics "
                :arguments 
                  ( ("observes" variable nil (reparse-symbol function_parameters) [8078 8086])
                    ("actions" variable nil (reparse-symbol function_parameters) [8088 8095])
                    ("advantages" variable nil (reparse-symbol function_parameters) [8097 8107])
                    ("disc_sum_rew" variable nil (reparse-symbol function_parameters) [8109 8121])
                    ("logger" variable nil (reparse-symbol function_parameters) [8123 8129])
                    ("episode" variable nil (reparse-symbol function_parameters) [8131 8138]))                  )
                nil [8058 9051])
            ("main" function
               (:documentation " Main training loop

    Args:
        env_name: OpenAI Gym environment name, e.g. 'Hopper-v1'
        num_episodes: maximum number of episodes to run
        gamma: reward discount factor (float)
        lam: lambda from Generalized Advantage Estimate
        kl_targ: D_KL target for policy update [D_KL(pi_old || pi_new)
        batch_size: number of episodes per policy training batch
        hid1_mult: hid1 size for policy and value_f (mutliplier of obs dimension)
        policy_logvar: natural log of initial policy variance
    "
                :arguments 
                  ( ("env_name" variable nil (reparse-symbol function_parameters) [9062 9070])
                    ("num_episodes" variable nil (reparse-symbol function_parameters) [9072 9084])
                    ("gamma" variable nil (reparse-symbol function_parameters) [9086 9091])
                    ("lam" variable nil (reparse-symbol function_parameters) [9093 9096])
                    ("kl_targ" variable nil (reparse-symbol function_parameters) [9098 9105])
                    ("batch_size" variable nil (reparse-symbol function_parameters) [9107 9117])
                    ("hid1_mult" variable nil (reparse-symbol function_parameters) [9119 9128])
                    ("policy_logvar" variable nil (reparse-symbol function_parameters) [9130 9143]))                  )
                nil [9053 11492])
            ("if" code nil nil [11494 12918]))          
      :file "train.py"
      :pointmax 12918
      :fsize 12917
      :lastmodtime '(23183 23806 0 0)
      :unmatched-syntax nil)
    (semanticdb-table "policy.py"
      :major-mode 'python-mode
      :tags 
        '( ("\"\"\"
NN Policy with KL Divergence Constraint (PPO / TRPO)

Written by Patrick Coady (pat-coady.github.io)
\"\"\"" code nil nil [1 109])
            ("numpy" include nil nil [110 128])
            ("tensorflow" include nil nil [129 152])
            ("Policy" type
               (:documentation " NN-based policy approximation "
                :superclasses ("object")
                :members 
                  ( ("__init__" function
                       (:suite 
                          ( ("\"\"\"
        Args:
            obs_dim: num observation dimensions (int)
            act_dim: num action dimensions (int)
            kl_targ: target KL divergence between pi_old and pi_new
            hid1_mult: size of first hidden layer, multiplier of obs_dim
            policy_logvar: natural log of initial policy variance
        \"\"\"" code nil (reparse-symbol indented_block_body) [304 643])
                            ("self" variable nil (reparse-symbol indented_block_body) [652 667])
                            ("self" variable nil (reparse-symbol indented_block_body) [721 734])
                            ("self" variable nil (reparse-symbol indented_block_body) [793 815])
                            ("self" variable nil (reparse-symbol indented_block_body) [824 850])
                            ("self" variable nil (reparse-symbol indented_block_body) [859 893])
                            ("self" variable nil (reparse-symbol indented_block_body) [902 918])
                            ("self" variable nil (reparse-symbol indented_block_body) [927 941])
                            ("self" variable nil (reparse-symbol indented_block_body) [950 974])
                            ("self" variable nil (reparse-symbol indented_block_body) [1033 1055])
                            ("self" variable nil (reparse-symbol indented_block_body) [1064 1086])
                            ("self" code nil (reparse-symbol indented_block_body) [1095 1114])
                            ("self" code nil (reparse-symbol indented_block_body) [1123 1143]))                          
                        :parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [236 240])
                            ("obs_dim" variable nil (reparse-symbol function_parameters) [242 249])
                            ("act_dim" variable nil (reparse-symbol function_parameters) [251 258])
                            ("kl_targ" variable nil (reparse-symbol function_parameters) [260 267])
                            ("hid1_mult" variable nil (reparse-symbol function_parameters) [269 278])
                            ("policy_logvar" variable nil (reparse-symbol function_parameters) [280 293]))                          
                        :documentation "
        Args:
            obs_dim: num observation dimensions (int)
            act_dim: num action dimensions (int)
            kl_targ: target KL divergence between pi_old and pi_new
            hid1_mult: size of first hidden layer, multiplier of obs_dim
            policy_logvar: natural log of initial policy variance
        "
                        :constructor-flag t)
                        (reparse-symbol indented_block_body) [223 1144])
                    ("_build_graph" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [1166 1170]))                          
                        :documentation " Build and initialize TensorFlow graph ")
                        (reparse-symbol indented_block_body) [1149 1530])
                    ("_placeholders" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [1553 1557]))                          
                        :documentation " Input placeholders")
                        (reparse-symbol indented_block_body) [1535 2381])
                    ("_policy_nn" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [2401 2405]))                          
                        :documentation " Neural net for policy approximation function

        Policy parameterized by Gaussian means and variances. NN outputs mean
         action based on observation. Trainable variables hold log-variances
         for each action dimension (i.e. variances not determined by NN).
        ")
                        (reparse-symbol indented_block_body) [2386 4715])
                    ("_logprob" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [4733 4737]))                          
                        :documentation " Calculate log probabilities of a batch of observations & actions

        Calculates log probabilities using previous step's model parameters and
        new parameters being trained.
        ")
                        (reparse-symbol indented_block_body) [4720 5428])
                    ("_kl_entropy" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [5449 5453]))                          
                        :documentation "
        Add to Graph:
            1. KL divergence between old and new distributions
            2. Entropy of present policy given states and actions

        https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Kullback.E2.80.93Leibler_divergence
        https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Entropy
        ")
                        (reparse-symbol indented_block_body) [5433 6472])
                    ("_sample" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [6489 6493]))                          
                        :documentation " Sample from distribution, given observation ")
                        (reparse-symbol indented_block_body) [6477 6724])
                    ("_loss_train_op" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [6748 6752]))                          
                        :documentation "
        Three loss terms:
            1) standard policy gradient
            2) D_KL(pi_old || pi_new)
            3) Hinge loss on [D_KL - kl_targ]^2

        See: https://arxiv.org/pdf/1707.02286.pdf
        ")
                        (reparse-symbol indented_block_body) [6729 7450])
                    ("_init_session" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [7473 7477]))                          
                        :documentation "Launch TensorFlow session and initialize variables")
                        (reparse-symbol indented_block_body) [7455 7623])
                    ("sample" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [7639 7643])
                            ("obs" variable nil (reparse-symbol function_parameters) [7645 7648]))                          
                        :documentation "Draw sample from policy distribution")
                        (reparse-symbol indented_block_body) [7628 7810])
                    ("update" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [7826 7830])
                            ("observes" variable nil (reparse-symbol function_parameters) [7832 7840])
                            ("actions" variable nil (reparse-symbol function_parameters) [7842 7849])
                            ("advantages" variable nil (reparse-symbol function_parameters) [7851 7861])
                            ("logger" variable nil (reparse-symbol function_parameters) [7863 7869]))                          
                        :documentation " Update policy based on observations, actions and advantages

        Args:
            observes: observations, shape = (N, obs_dim)
            actions: actions, shape = (N, act_dim)
            advantages: advantages, shape = (N,)
            logger: Logger object, see utils.py
        ")
                        (reparse-symbol indented_block_body) [7815 9941])
                    ("close_sess" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [9961 9965]))                          
                        :documentation " Close TensorFlow session ")
                        (reparse-symbol indented_block_body) [9946 10035]))                  
                :type "class")
                nil [155 10035]))          
      :file "policy.py"
      :pointmax 10035
      :fsize 10034
      :lastmodtime '(23183 25164 0 0)
      :unmatched-syntax nil)
    (semanticdb-table "value_function.py"
      :major-mode 'python-mode
      :tags 
        '( ("\"\"\"
State-Value Function

Written by Patrick Coady (pat-coady.github.io)
\"\"\"" code nil nil [1 77])
            ("tensorflow" include nil nil [79 102])
            ("numpy" include nil nil [103 121])
            ("sklearn.utils" include nil nil [122 155])
            ("NNValueFunction" type
               (:documentation " NN-based state-value function "
                :superclasses ("object")
                :members 
                  ( ("__init__" function
                       (:suite 
                          ( ("\"\"\"
        Args:
            obs_dim: number of dimensions in observation vector (int)
            hid1_mult: size of first hidden layer, multiplier of obs_dim
        \"\"\"" code nil (reparse-symbol indented_block_body) [283 455])
                            ("self" variable nil (reparse-symbol indented_block_body) [464 491])
                            ("self" variable nil (reparse-symbol indented_block_body) [500 527])
                            ("self" variable nil (reparse-symbol indented_block_body) [536 558])
                            ("self" variable nil (reparse-symbol indented_block_body) [567 593])
                            ("self" variable nil (reparse-symbol indented_block_body) [602 618])
                            ("self" variable nil (reparse-symbol indented_block_body) [627 641])
                            ("self" code nil (reparse-symbol indented_block_body) [689 708])
                            ("self" variable nil (reparse-symbol indented_block_body) [717 753])
                            ("self" code nil (reparse-symbol indented_block_body) [762 786]))                          
                        :parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [248 252])
                            ("obs_dim" variable nil (reparse-symbol function_parameters) [254 261])
                            ("hid1_mult" variable nil (reparse-symbol function_parameters) [263 272]))                          
                        :documentation "
        Args:
            obs_dim: number of dimensions in observation vector (int)
            hid1_mult: size of first hidden layer, multiplier of obs_dim
        "
                        :constructor-flag t)
                        (reparse-symbol indented_block_body) [235 787])
                    ("_build_graph" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [809 813]))                          
                        :documentation " Construct TensorFlow graph, including loss function, init op and train op ")
                        (reparse-symbol indented_block_body) [792 3087])
                    ("fit" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [3100 3104])
                            ("x" variable nil (reparse-symbol function_parameters) [3106 3107])
                            ("y" variable nil (reparse-symbol function_parameters) [3109 3110])
                            ("logger" variable nil (reparse-symbol function_parameters) [3112 3118]))                          
                        :documentation " Fit model to current data batch + previous data batch

        Args:
            x: features
            y: target
            logger: logger to save training loss and % explained variance
        ")
                        (reparse-symbol indented_block_body) [3092 4625])
                    ("predict" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [4642 4646])
                            ("x" variable nil (reparse-symbol function_parameters) [4648 4649]))                          
                        :documentation " Predict method ")
                        (reparse-symbol indented_block_body) [4630 4815])
                    ("close_sess" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [4835 4839]))                          
                        :documentation " Close TensorFlow session ")
                        (reparse-symbol indented_block_body) [4820 4909]))                  
                :type "class")
                nil [158 4909]))          
      :file "value_function.py"
      :pointmax 4909
      :fsize 4908
      :lastmodtime '(23183 44693 0 0)
      :unmatched-syntax nil)
    (semanticdb-table "utils.py"
      :file "utils.py"
      :fsize 4103
      :lastmodtime '(23029 57982 0 0))
    (semanticdb-table "plotting.py"
      :major-mode 'python-mode
      :tags 
        '( ("\"\"\"
Short Plotting Routine to Plot Pandas Dataframes by Column Label

1. Takes list of dateframes to compare multiple trials
2. Takes list of y-variables to combine on 1 plot
3. Legend location and y-axis limits can be customized

Written by Patrick Coady (pat-coady.github.io)
\"\"\"" code nil nil [1 282])
            ("matplotlib.pyplot" include nil nil [283 314])
            ("df_plot" function
               (:documentation " Plot y vs. x curves from pandas dataframe(s)

    Args:
        dfs: list of pandas dataframes
        x: str column label for x variable
        ys: list of str column labels for y variable(s)
        ylim: tuple to override automatic y-axis limits
        legend_loc: str to override automatic legend placement:
            'upper left', 'lower left', 'lower right' , 'right' ,
            'center left', 'center right', 'lower center',
            'upper center', and 'center'
    "
                :arguments 
                  ( ("dfs" variable nil (reparse-symbol function_parameters) [329 332])
                    ("x" variable nil (reparse-symbol function_parameters) [334 335])
                    ("ys" variable nil (reparse-symbol function_parameters) [337 339])
                    ("ylim" variable nil (reparse-symbol function_parameters) [341 345])
                    ("legend_loc" variable nil (reparse-symbol function_parameters) [352 362]))                  )
                nil [317 1213]))          
      :file "plotting.py"
      :pointmax 1213
      :fsize 1212
      :lastmodtime '(23029 57982 0 0)
      :unmatched-syntax nil))
  :file "!Users!xiaoli!Google Drive!PHD_research!projects!docker!docker_home!rlfps!sandbox!trpo-master!src!semantic.cache"
  :semantic-tag-version "2.0"
  :semanticdb-version "2.2")
