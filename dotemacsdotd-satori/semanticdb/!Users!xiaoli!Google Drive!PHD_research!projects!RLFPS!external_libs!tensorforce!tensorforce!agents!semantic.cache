;; Object agents/
;; SEMANTICDB Tags save file
(semanticdb-project-database-file "agents/"
  :tables
  (list
    (semanticdb-table "ppo_agent.py"
      :major-mode 'python-mode
      :tags 
        '( ("__future__" include nil nil [681 719])
            ("__future__" include nil nil [720 757])
            ("__future__" include nil nil [758 789])
            ("tensorforce" include nil nil [791 831])
            ("tensorforce.agents" include nil nil [832 873])
            ("tensorforce.models" include nil nil [874 921])
            ("PPOAgent" type
               (:documentation "
    Proximal Policy Optimization agent ([Schulman et al., 2017]
    (https://openai-public.s3-us-west-2.amazonaws.com/blog/2017-07/ppo/ppo-arxiv.pdf).
    "
                :superclasses ("BatchAgent")
                :members 
                  ( ("__init__" function
                       (:suite 
                          ( ("\"\"\"
        Creates a proximal policy optimization agent (PPO), ([Schulman et al., 2017]
        (https://openai-public.s3-us-west-2.amazonaws.com/blog/2017-07/ppo/ppo-arxiv.pdf).

        Args:
            states_spec: Dict containing at least one state definition. In the case of a single state,
               keys `shape` and `type` are necessary. For multiple states, pass a dict of dicts where each state
               is a dict itself with a unique name as its key.
            actions_spec: Dict containing at least one action definition. Actions have types and either `num_actions`
                for discrete actions or a `shape` for continuous actions. Consult documentation and tests for more.
            network_spec: List of layers specifying a neural network via layer types, sizes and optional arguments
                such as activation or regularisation. Full examples are in the examples/configs folder.
            device: Device string specifying model device.
            session_config: optional tf.ConfigProto with additional desired session configurations
            scope: TensorFlow scope, defaults to agent name (e.g. `dqn`).
            saver_spec: Dict specifying automated saving. Use `directory` to specify where checkpoints are saved. Use
                either `seconds` or `steps` to specify how often the model should be saved. The `load` flag specifies
                if a model is initially loaded (set to True) from a file `file`.
            summary_spec: Dict specifying summaries for TensorBoard. Requires a 'directory' to store summaries, `steps`
                or `seconds` to specify how often to save summaries, and a list of `labels` to indicate which values
                to export, e.g. `losses`, `variables`. Consult neural network class and model for all available labels.
            distributed_spec: Dict specifying distributed functionality. Use `parameter_server` and `replica_model`
                Boolean flags to indicate workers and parameter servers. Use a `cluster_spec` key to pass a TensorFlow
                cluster spec.
            discount: Float specifying reward discount factor.
            variable_noise: Experimental optional parameter specifying variable noise (NoisyNet).
            states_preprocessing_spec: Optional list of states preprocessors to apply to state  
                (e.g. `image_resize`, `grayscale`).
            explorations_spec: Optional dict specifying action exploration type (epsilon greedy  
                or Gaussian noise).
            reward_preprocessing_spec: Optional dict specifying reward preprocessing.
            distributions_spec: Optional dict specifying action distributions to override default distribution choices.
                Must match action names.
            entropy_regularization: Optional positive float specifying an entropy regularization value.
            baseline_mode: String specifying baseline mode, `states` for a separate baseline per state, `network`
                for sharing parameters with the training network.
            baseline: Optional dict specifying baseline type (e.g. `mlp`, `cnn`), and its layer sizes. Consult
                examples/configs for full example configurations.
            baseline_optimizer: Optional dict specifying an optimizer and its parameters for the baseline following
                the same conventions as the main optimizer.
            gae_lambda: Optional float specifying lambda parameter for generalized advantage estimation.
            batched_observe: Optional int specifying how many observe calls are batched into one session run.
                Without batching, throughput will be lower because every `observe` triggers a session invocation to
                update rewards in the graph.
            batch_size: Int specifying number of samples collected via `observe` before an update is executed.
            keep_last_timestep: Boolean flag specifying whether last sample is kept, default True.
            likelihood_ratio_clipping: Optional clipping of likelihood ratio between old and new policy.
            step_optimizer: Optimizer dict specification for optimizer used in each PPO update step, defaults to
                Adam if None.
            optimization_steps: Int specifying number of optimization steps to execute on the collected batch using
                the step optimizer.                `
        \"\"\"" code nil (reparse-symbol indented_block_body) [1988 6428])
                            ("if" code nil (reparse-symbol indented_block_body) [6437 6526])
                            ("self" variable nil (reparse-symbol indented_block_body) [6535 6567])
                            ("self" variable nil (reparse-symbol indented_block_body) [6576 6596])
                            ("self" variable nil (reparse-symbol indented_block_body) [6605 6641])
                            ("self" variable nil (reparse-symbol indented_block_body) [6650 6668])
                            ("self" variable nil (reparse-symbol indented_block_body) [6677 6705])
                            ("self" variable nil (reparse-symbol indented_block_body) [6714 6746])
                            ("self" variable nil (reparse-symbol indented_block_body) [6755 6795])
                            ("self" variable nil (reparse-symbol indented_block_body) [6804 6828])
                            ("self" variable nil (reparse-symbol indented_block_body) [6837 6873])
                            ("self" variable nil (reparse-symbol indented_block_body) [6882 6940])
                            ("self" variable nil (reparse-symbol indented_block_body) [6949 6991])
                            ("self" variable nil (reparse-symbol indented_block_body) [7000 7058])
                            ("self" variable nil (reparse-symbol indented_block_body) [7067 7111])
                            ("self" variable nil (reparse-symbol indented_block_body) [7120 7172])
                            ("self" variable nil (reparse-symbol indented_block_body) [7181 7215])
                            ("self" variable nil (reparse-symbol indented_block_body) [7224 7248])
                            ("self" variable nil (reparse-symbol indented_block_body) [7257 7301])
                            ("self" variable nil (reparse-symbol indented_block_body) [7310 7338])
                            ("self" variable nil (reparse-symbol indented_block_body) [7347 7405])
                            ("if" code nil (reparse-symbol indented_block_body) [7415 7555])
                            ("self" variable nil (reparse-symbol indented_block_body) [7564 7706])
                            ("super" code nil (reparse-symbol indented_block_body) [7716 7963]))                          
                        :parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [1146 1150])
                            ("states_spec" variable nil (reparse-symbol function_parameters) [1160 1171])
                            ("actions_spec" variable nil (reparse-symbol function_parameters) [1181 1193])
                            ("network_spec" variable nil (reparse-symbol function_parameters) [1203 1215])
                            ("device" variable nil (reparse-symbol function_parameters) [1225 1231])
                            ("session_config" variable nil (reparse-symbol function_parameters) [1246 1260])
                            ("scope" variable nil (reparse-symbol function_parameters) [1275 1280])
                            ("saver_spec" variable nil (reparse-symbol function_parameters) [1296 1306])
                            ("summary_spec" variable nil (reparse-symbol function_parameters) [1321 1333])
                            ("distributed_spec" variable nil (reparse-symbol function_parameters) [1348 1364])
                            ("discount" variable nil (reparse-symbol function_parameters) [1379 1387])
                            ("variable_noise" variable nil (reparse-symbol function_parameters) [1402 1416])
                            ("states_preprocessing_spec" variable nil (reparse-symbol function_parameters) [1431 1456])
                            ("explorations_spec" variable nil (reparse-symbol function_parameters) [1471 1488])
                            ("reward_preprocessing_spec" variable nil (reparse-symbol function_parameters) [1503 1528])
                            ("distributions_spec" variable nil (reparse-symbol function_parameters) [1543 1561])
                            ("entropy_regularization" variable nil (reparse-symbol function_parameters) [1576 1598])
                            ("baseline_mode" variable nil (reparse-symbol function_parameters) [1613 1626])
                            ("baseline" variable nil (reparse-symbol function_parameters) [1641 1649])
                            ("baseline_optimizer" variable nil (reparse-symbol function_parameters) [1664 1682])
                            ("gae_lambda" variable nil (reparse-symbol function_parameters) [1697 1707])
                            ("batched_observe" variable nil (reparse-symbol function_parameters) [1722 1737])
                            ("batch_size" variable nil (reparse-symbol function_parameters) [1752 1762])
                            ("keep_last_timestep" variable nil (reparse-symbol function_parameters) [1777 1795])
                            ("likelihood_ratio_clipping" variable nil (reparse-symbol function_parameters) [1810 1835])
                            ("step_optimizer" variable nil (reparse-symbol function_parameters) [1850 1864])
                            ("optimization_steps" variable nil (reparse-symbol function_parameters) [1879 1897]))                          
                        :documentation "
        Creates a proximal policy optimization agent (PPO), ([Schulman et al., 2017]
        (https://openai-public.s3-us-west-2.amazonaws.com/blog/2017-07/ppo/ppo-arxiv.pdf).

        Args:
            states_spec: Dict containing at least one state definition. In the case of a single state,
               keys `shape` and `type` are necessary. For multiple states, pass a dict of dicts where each state
               is a dict itself with a unique name as its key.
            actions_spec: Dict containing at least one action definition. Actions have types and either `num_actions`
                for discrete actions or a `shape` for continuous actions. Consult documentation and tests for more.
            network_spec: List of layers specifying a neural network via layer types, sizes and optional arguments
                such as activation or regularisation. Full examples are in the examples/configs folder.
            device: Device string specifying model device.
            session_config: optional tf.ConfigProto with additional desired session configurations
            scope: TensorFlow scope, defaults to agent name (e.g. `dqn`).
            saver_spec: Dict specifying automated saving. Use `directory` to specify where checkpoints are saved. Use
                either `seconds` or `steps` to specify how often the model should be saved. The `load` flag specifies
                if a model is initially loaded (set to True) from a file `file`.
            summary_spec: Dict specifying summaries for TensorBoard. Requires a 'directory' to store summaries, `steps`
                or `seconds` to specify how often to save summaries, and a list of `labels` to indicate which values
                to export, e.g. `losses`, `variables`. Consult neural network class and model for all available labels.
            distributed_spec: Dict specifying distributed functionality. Use `parameter_server` and `replica_model`
                Boolean flags to indicate workers and parameter servers. Use a `cluster_spec` key to pass a TensorFlow
                cluster spec.
            discount: Float specifying reward discount factor.
            variable_noise: Experimental optional parameter specifying variable noise (NoisyNet).
            states_preprocessing_spec: Optional list of states preprocessors to apply to state  
                (e.g. `image_resize`, `grayscale`).
            explorations_spec: Optional dict specifying action exploration type (epsilon greedy  
                or Gaussian noise).
            reward_preprocessing_spec: Optional dict specifying reward preprocessing.
            distributions_spec: Optional dict specifying action distributions to override default distribution choices.
                Must match action names.
            entropy_regularization: Optional positive float specifying an entropy regularization value.
            baseline_mode: String specifying baseline mode, `states` for a separate baseline per state, `network`
                for sharing parameters with the training network.
            baseline: Optional dict specifying baseline type (e.g. `mlp`, `cnn`), and its layer sizes. Consult
                examples/configs for full example configurations.
            baseline_optimizer: Optional dict specifying an optimizer and its parameters for the baseline following
                the same conventions as the main optimizer.
            gae_lambda: Optional float specifying lambda parameter for generalized advantage estimation.
            batched_observe: Optional int specifying how many observe calls are batched into one session run.
                Without batching, throughput will be lower because every `observe` triggers a session invocation to
                update rewards in the graph.
            batch_size: Int specifying number of samples collected via `observe` before an update is executed.
            keep_last_timestep: Boolean flag specifying whether last sample is kept, default True.
            likelihood_ratio_clipping: Optional clipping of likelihood ratio between old and new policy.
            step_optimizer: Optimizer dict specification for optimizer used in each PPO update step, defaults to
                Adam if None.
            optimization_steps: Int specifying number of optimization steps to execute on the collected batch using
                the step optimizer.                `
        "
                        :constructor-flag t)
                        (reparse-symbol indented_block_body) [1124 7964])
                    ("initialize_model" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [7990 7994]))                          )
                        (reparse-symbol indented_block_body) [7969 9099]))                  
                :type "class")
                nil [924 9099]))          
      :file "ppo_agent.py"
      :pointmax 9099
      :fsize 9098
      :lastmodtime '(23097 11177 0 0)
      :unmatched-syntax nil)
    (semanticdb-table "trpo_agent.py"
      :major-mode 'python-mode
      :tags 
        '( ("__future__" include nil nil [681 719])
            ("__future__" include nil nil [720 757])
            ("__future__" include nil nil [758 789])
            ("tensorforce" include nil nil [791 831])
            ("tensorforce.agents" include nil nil [832 873])
            ("tensorforce.models" include nil nil [874 921])
            ("TRPOAgent" type
               (:documentation "
    Trust Region Policy Optimization ([Schulman et al., 2015](https://arxiv.org/abs/1502.05477)) agent.
    "
                :superclasses ("BatchAgent")
                :members 
                  ( ("__init__" function
                       (:suite 
                          ( ("\"\"\"
        Creates a Trust Region Policy Optimization ([Schulman et al., 2015](https://arxiv.org/abs/1502.05477)) agent.

        Args:
            states_spec: Dict containing at least one state definition. In the case of a single state,
               keys `shape` and `type` are necessary. For multiple states, pass a dict of dicts where each state
               is a dict itself with a unique name as its key.
            actions_spec: Dict containing at least one action definition. Actions have types and either `num_actions`
                for discrete actions or a `shape` for continuous actions. Consult documentation and tests for more.
            network_spec: List of layers specifying a neural network via layer types, sizes and optional arguments
                such as activation or regularisation. Full examples are in the examples/configs folder.
            device: Device string specifying model device.
            session_config: optional tf.ConfigProto with additional desired session configurations
            scope: TensorFlow scope, defaults to agent name (e.g. `dqn`).
            saver_spec: Dict specifying automated saving. Use `directory` to specify where checkpoints are saved. Use
                either `seconds` or `steps` to specify how often the model should be saved. The `load` flag specifies
                if a model is initially loaded (set to True) from a file `file`.
            summary_spec: Dict specifying summaries for TensorBoard. Requires a 'directory' to store summaries, `steps`
                or `seconds` to specify how often to save summaries, and a list of `labels` to indicate which values
                to export, e.g. `losses`, `variables`. Consult neural network class and model for all available labels.
            distributed_spec: Dict specifying distributed functionality. Use `parameter_server` and `replica_model`
                Boolean flags to indicate workers and parameter servers. Use a `cluster_spec` key to pass a TensorFlow
                cluster spec.
            discount: Float specifying reward discount factor.
            variable_noise: Experimental optional parameter specifying variable noise (NoisyNet).
            states_preprocessing_spec: Optional list of states preprocessors to apply to state  
                (e.g. `image_resize`, `grayscale`).
            explorations_spec: Optional dict specifying action exploration type (epsilon greedy  
                or Gaussian noise).
            reward_preprocessing_spec: Optional dict specifying reward preprocessing.
            distributions_spec: Optional dict specifying action distributions to override default distribution choices.
                Must match action names.
            entropy_regularization: Optional positive float specifying an entropy regularization value.
            baseline_mode: String specifying baseline mode, `states` for a separate baseline per state, `network`
                for sharing parameters with the training network.
            baseline: Optional dict specifying baseline type (e.g. `mlp`, `cnn`), and its layer sizes. Consult
             examples/configs for full example configurations.
            baseline_optimizer: Optional dict specifying an optimizer and its parameters for the baseline
                following the same conventions as the main optimizer.
            gae_lambda: Optional float specifying lambda parameter for generalized advantage estimation.
            batched_observe: Optional int specifying how many observe calls are batched into one session run.
                Without batching, throughput will be lower because every `observe` triggers a session invocation to
                update rewards in the graph.
            batch_size: Int specifying number of samples collected via `observe` before an update is executed.
            keep_last_timestep: Boolean flag specifying whether last sample is kept, default True.
            likelihood_ratio_clipping: Optional clipping of likelihood ratio between old and new policy.
            learning_rate: Learning rate which may be interpreted differently according to optimizer, e.g. a natural
                gradient optimizer interprets the learning rate as the max kl-divergence between old and updated policy.
            cg_max_iterations: Int > 0 specifying conjugate gradient iterations, typically 10-20 are sufficient to
                find effective approximate solutions.
            cg_damping: Conjugate gradient damping value to increase numerical stability.
            cg_unroll_loop: Boolean indicating whether loop unrolling in TensorFlow is to be used which seems to
                impact performance negatively at this point, default False.
        \"\"\"" code nil (reparse-symbol indented_block_body) [1924 6677])
                            ("if" code nil (reparse-symbol indented_block_body) [6686 6775])
                            ("self" variable nil (reparse-symbol indented_block_body) [6784 7284])
                            ("self" variable nil (reparse-symbol indented_block_body) [7294 7326])
                            ("self" variable nil (reparse-symbol indented_block_body) [7335 7355])
                            ("self" variable nil (reparse-symbol indented_block_body) [7364 7400])
                            ("self" variable nil (reparse-symbol indented_block_body) [7409 7427])
                            ("self" variable nil (reparse-symbol indented_block_body) [7436 7464])
                            ("self" variable nil (reparse-symbol indented_block_body) [7473 7505])
                            ("self" variable nil (reparse-symbol indented_block_body) [7514 7554])
                            ("self" variable nil (reparse-symbol indented_block_body) [7563 7587])
                            ("self" variable nil (reparse-symbol indented_block_body) [7596 7632])
                            ("self" variable nil (reparse-symbol indented_block_body) [7641 7699])
                            ("self" variable nil (reparse-symbol indented_block_body) [7708 7750])
                            ("self" variable nil (reparse-symbol indented_block_body) [7759 7817])
                            ("self" variable nil (reparse-symbol indented_block_body) [7826 7870])
                            ("self" variable nil (reparse-symbol indented_block_body) [7879 7931])
                            ("self" variable nil (reparse-symbol indented_block_body) [7940 7974])
                            ("self" variable nil (reparse-symbol indented_block_body) [7983 8007])
                            ("self" variable nil (reparse-symbol indented_block_body) [8016 8060])
                            ("self" variable nil (reparse-symbol indented_block_body) [8069 8097])
                            ("self" variable nil (reparse-symbol indented_block_body) [8106 8164])
                            ("super" code nil (reparse-symbol indented_block_body) [8174 8422]))                          
                        :parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [1100 1104])
                            ("states_spec" variable nil (reparse-symbol function_parameters) [1114 1125])
                            ("actions_spec" variable nil (reparse-symbol function_parameters) [1135 1147])
                            ("network_spec" variable nil (reparse-symbol function_parameters) [1157 1169])
                            ("device" variable nil (reparse-symbol function_parameters) [1179 1185])
                            ("session_config" variable nil (reparse-symbol function_parameters) [1200 1214])
                            ("scope" variable nil (reparse-symbol function_parameters) [1229 1234])
                            ("saver_spec" variable nil (reparse-symbol function_parameters) [1251 1261])
                            ("summary_spec" variable nil (reparse-symbol function_parameters) [1276 1288])
                            ("distributed_spec" variable nil (reparse-symbol function_parameters) [1303 1319])
                            ("discount" variable nil (reparse-symbol function_parameters) [1334 1342])
                            ("variable_noise" variable nil (reparse-symbol function_parameters) [1357 1371])
                            ("states_preprocessing_spec" variable nil (reparse-symbol function_parameters) [1386 1411])
                            ("explorations_spec" variable nil (reparse-symbol function_parameters) [1426 1443])
                            ("reward_preprocessing_spec" variable nil (reparse-symbol function_parameters) [1458 1483])
                            ("distributions_spec" variable nil (reparse-symbol function_parameters) [1498 1516])
                            ("entropy_regularization" variable nil (reparse-symbol function_parameters) [1531 1553])
                            ("baseline_mode" variable nil (reparse-symbol function_parameters) [1568 1581])
                            ("baseline" variable nil (reparse-symbol function_parameters) [1596 1604])
                            ("baseline_optimizer" variable nil (reparse-symbol function_parameters) [1619 1637])
                            ("gae_lambda" variable nil (reparse-symbol function_parameters) [1652 1662])
                            ("batched_observe" variable nil (reparse-symbol function_parameters) [1677 1692])
                            ("batch_size" variable nil (reparse-symbol function_parameters) [1707 1717])
                            ("keep_last_timestep" variable nil (reparse-symbol function_parameters) [1732 1750])
                            ("likelihood_ratio_clipping" variable nil (reparse-symbol function_parameters) [1765 1790])
                            ("learning_rate" variable nil (reparse-symbol function_parameters) [1805 1818])
                            ("cg_max_iterations" variable nil (reparse-symbol function_parameters) [1833 1850])
                            ("cg_damping" variable nil (reparse-symbol function_parameters) [1863 1873])
                            ("cg_unroll_loop" variable nil (reparse-symbol function_parameters) [1888 1902]))                          
                        :documentation "
        Creates a Trust Region Policy Optimization ([Schulman et al., 2015](https://arxiv.org/abs/1502.05477)) agent.

        Args:
            states_spec: Dict containing at least one state definition. In the case of a single state,
               keys `shape` and `type` are necessary. For multiple states, pass a dict of dicts where each state
               is a dict itself with a unique name as its key.
            actions_spec: Dict containing at least one action definition. Actions have types and either `num_actions`
                for discrete actions or a `shape` for continuous actions. Consult documentation and tests for more.
            network_spec: List of layers specifying a neural network via layer types, sizes and optional arguments
                such as activation or regularisation. Full examples are in the examples/configs folder.
            device: Device string specifying model device.
            session_config: optional tf.ConfigProto with additional desired session configurations
            scope: TensorFlow scope, defaults to agent name (e.g. `dqn`).
            saver_spec: Dict specifying automated saving. Use `directory` to specify where checkpoints are saved. Use
                either `seconds` or `steps` to specify how often the model should be saved. The `load` flag specifies
                if a model is initially loaded (set to True) from a file `file`.
            summary_spec: Dict specifying summaries for TensorBoard. Requires a 'directory' to store summaries, `steps`
                or `seconds` to specify how often to save summaries, and a list of `labels` to indicate which values
                to export, e.g. `losses`, `variables`. Consult neural network class and model for all available labels.
            distributed_spec: Dict specifying distributed functionality. Use `parameter_server` and `replica_model`
                Boolean flags to indicate workers and parameter servers. Use a `cluster_spec` key to pass a TensorFlow
                cluster spec.
            discount: Float specifying reward discount factor.
            variable_noise: Experimental optional parameter specifying variable noise (NoisyNet).
            states_preprocessing_spec: Optional list of states preprocessors to apply to state  
                (e.g. `image_resize`, `grayscale`).
            explorations_spec: Optional dict specifying action exploration type (epsilon greedy  
                or Gaussian noise).
            reward_preprocessing_spec: Optional dict specifying reward preprocessing.
            distributions_spec: Optional dict specifying action distributions to override default distribution choices.
                Must match action names.
            entropy_regularization: Optional positive float specifying an entropy regularization value.
            baseline_mode: String specifying baseline mode, `states` for a separate baseline per state, `network`
                for sharing parameters with the training network.
            baseline: Optional dict specifying baseline type (e.g. `mlp`, `cnn`), and its layer sizes. Consult
             examples/configs for full example configurations.
            baseline_optimizer: Optional dict specifying an optimizer and its parameters for the baseline
                following the same conventions as the main optimizer.
            gae_lambda: Optional float specifying lambda parameter for generalized advantage estimation.
            batched_observe: Optional int specifying how many observe calls are batched into one session run.
                Without batching, throughput will be lower because every `observe` triggers a session invocation to
                update rewards in the graph.
            batch_size: Int specifying number of samples collected via `observe` before an update is executed.
            keep_last_timestep: Boolean flag specifying whether last sample is kept, default True.
            likelihood_ratio_clipping: Optional clipping of likelihood ratio between old and new policy.
            learning_rate: Learning rate which may be interpreted differently according to optimizer, e.g. a natural
                gradient optimizer interprets the learning rate as the max kl-divergence between old and updated policy.
            cg_max_iterations: Int > 0 specifying conjugate gradient iterations, typically 10-20 are sufficient to
                find effective approximate solutions.
            cg_damping: Conjugate gradient damping value to increase numerical stability.
            cg_unroll_loop: Boolean indicating whether loop unrolling in TensorFlow is to be used which seems to
                impact performance negatively at this point, default False.
        "
                        :constructor-flag t)
                        (reparse-symbol indented_block_body) [1078 8423])
                    ("initialize_model" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [8449 8453]))                          )
                        (reparse-symbol indented_block_body) [8428 9558]))                  
                :type "class")
                nil [924 9558]))          
      :file "trpo_agent.py"
      :pointmax 9558
      :fsize 9557
      :lastmodtime '(23097 11177 0 0)
      :unmatched-syntax nil)
    (semanticdb-table "agent.py"
      :major-mode 'python-mode
      :tags 
        '( ("__future__" include nil nil [681 719])
            ("__future__" include nil nil [720 757])
            ("__future__" include nil nil [758 789])
            ("copy" include nil nil [791 816])
            ("numpy" include nil nil [818 836])
            ("inspect" include nil nil [837 851])
            ("tensorforce" include nil nil [853 899])
            ("tensorforce.agents" include nil nil [900 925])
            ("tensorforce.meta_parameter_recorder" include nil nil [926 995])
            ("Agent" type
               (:documentation "
    Basic Reinforcement learning agent. An agent encapsulates execution logic
    of a particular reinforcement learning algorithm and defines the external interface
    to the environment.

    The agent hence acts an intermediate layer between environment
    and backend execution (value function or policy updates).

    "
                :superclasses ("object")
                :members 
                  ( ("__init__" function
                       (:suite 
                          ( ("\"\"\"
        Initializes the reinforcement learning agent.

        Args:
            states_spec: Dict containing at least one state definition. In the case of a single state,
               keys `shape` and `type` are necessary. For multiple states, pass a dict of dicts where each state
               is a dict itself with a unique name as its key.
            actions_spec: Dict containing at least one action definition. Actions have types and either `num_actions`
                for discrete actions or a `shape` for continuous actions. Consult documentation and tests for more.
            batched_observe: Optional int specifying how many observe calls are batched into one session run.
                Without batching, throughput will be lower because every `observe` triggers a session invocation to
                update rewards in the graph.
        \"\"\"" code nil (reparse-symbol indented_block_body) [1471 2339])
                            ("self" variable nil (reparse-symbol indented_block_body) [2349 2393])
                            ("if" code nil (reparse-symbol indented_block_body) [2402 2474])
                            ("self" variable nil (reparse-symbol indented_block_body) [2483 2523])
                            ("for" code nil (reparse-symbol indented_block_body) [2532 2834])
                            ("self" variable nil (reparse-symbol indented_block_body) [2884 2909])
                            ("self" variable nil (reparse-symbol indented_block_body) [2918 2963])
                            ("if" code nil (reparse-symbol indented_block_body) [2972 3048])
                            ("self" variable nil (reparse-symbol indented_block_body) [3056 3098])
                            ("for" code nil (reparse-symbol indented_block_body) [3108 3855])
                            ("if" code nil (reparse-symbol indented_block_body) [3943 4102])
                            ("self" variable nil (reparse-symbol indented_block_body) [4112 4143])
                            ("if" code nil (reparse-symbol indented_block_body) [4254 4994])
                            ("self" variable nil (reparse-symbol indented_block_body) [5111 5147])
                            ("self" variable nil (reparse-symbol indented_block_body) [5219 5257])
                            ("if" code nil (reparse-symbol indented_block_body) [5266 5387])
                            ("self" code nil (reparse-symbol indented_block_body) [5396 5408]))                          
                        :parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [1383 1387])
                            ("states_spec" variable nil (reparse-symbol function_parameters) [1397 1408])
                            ("actions_spec" variable nil (reparse-symbol function_parameters) [1418 1430])
                            ("batched_observe" variable nil (reparse-symbol function_parameters) [1440 1455]))                          
                        :documentation "
        Initializes the reinforcement learning agent.

        Args:
            states_spec: Dict containing at least one state definition. In the case of a single state,
               keys `shape` and `type` are necessary. For multiple states, pass a dict of dicts where each state
               is a dict itself with a unique name as its key.
            actions_spec: Dict containing at least one action definition. Actions have types and either `num_actions`
                for discrete actions or a `shape` for continuous actions. Consult documentation and tests for more.
            batched_observe: Optional int specifying how many observe calls are batched into one session run.
                Without batching, throughput will be lower because every `observe` triggers a session invocation to
                update rewards in the graph.
        "
                        :constructor-flag t)
                        (reparse-symbol indented_block_body) [1361 5409])
                    ("__str__" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [5426 5430]))                          )
                        (reparse-symbol indented_block_body) [5414 5477])
                    ("close" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [5492 5496]))                          )
                        (reparse-symbol indented_block_body) [5482 5526])
                    ("initialize_model" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [5552 5556]))                          
                        :documentation "
        Creates the model for the respective agent based on specifications given by user. This is a separate
        call after constructing the agent because the agent constructor has to perform a number of checks
        on the specs first, sometimes adjusting them e.g. by converting to a dict.
        ")
                        (reparse-symbol indented_block_body) [5531 5915])
                    ("reset" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [5930 5934]))                          
                        :documentation "
        Reset the agent to its initial state on episode start. Updates internal episode and  
        timestep counter, internal states,  and resets preprocessors.
        ")
                        (reparse-symbol indented_block_body) [5920 6256])
                    ("act" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [6422 6426])
                            ("states" variable nil (reparse-symbol function_parameters) [6428 6434])
                            ("deterministic" variable nil (reparse-symbol function_parameters) [6436 6449]))                          
                        :documentation "
        Return action(s) for given state(s). States preprocessing and exploration are applied if  
        configured accordingly.

        Args:
            states: One state (usually a value tuple) or dict of states if multiple states are expected.
            deterministic: If true, no exploration and sampling is applied.
        Returns:
            Scalar value of the action or dict of multiple actions the agent wants to execute.

        ")
                        (reparse-symbol indented_block_body) [6414 7561])
                    ("observe" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [7578 7582])
                            ("terminal" variable nil (reparse-symbol function_parameters) [7584 7592])
                            ("reward" variable nil (reparse-symbol function_parameters) [7594 7600]))                          
                        :documentation "
        Observe experience from the environment to learn from. Optionally preprocesses rewards
        Child classes should call super to get the processed reward
        EX: terminal, reward = super()...

        Args:
            terminal: boolean indicating if the episode terminated after the observation.
            reward: scalar reward that resulted from executing the action.
        ")
                        (reparse-symbol indented_block_body) [7566 8875])
                    ("should_stop" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [8896 8900]))                          )
                        (reparse-symbol indented_block_body) [8880 8961])
                    ("last_observation" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [8987 8991]))                          )
                        (reparse-symbol indented_block_body) [8966 9236])
                    ("save_model" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [9256 9260])
                            ("directory" variable nil (reparse-symbol function_parameters) [9262 9271])
                            ("append_timestep" variable nil (reparse-symbol function_parameters) [9278 9293]))                          
                        :documentation "
        Save TensorFlow model. If no checkpoint directory is given, the model's default saver  
        directory is used. Optionally appends current timestep to prevent overwriting previous  
        checkpoint files. Turn off to be able to load model from the same given path argument as  
        given here.

        Args:
            directory: Optional checkpoint directory.
            use_global_step:  Appends the current timestep to the checkpoint file if true.
            If this is set to True, the load path must include the checkpoint timestep suffix.  
            For example, if stored to models/ and set to true, the exported file will be of the  
            form models/model.ckpt-X where X is the last timestep saved. The load path must  
            precisely match this file name. If this option is turned off, the checkpoint will  
            always overwrite the file specified in path and the model can always be loaded under  
            this path.

        Returns:
            Checkpoint path were the model was saved.
        ")
                        (reparse-symbol indented_block_body) [9241 10461])
                    ("restore_model" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [10484 10488])
                            ("directory" variable nil (reparse-symbol function_parameters) [10490 10499])
                            ("file" variable nil (reparse-symbol function_parameters) [10506 10510]))                          
                        :documentation "
        Restore TensorFlow model. If no checkpoint file is given, the latest checkpoint is  
        restored. If no checkpoint directory is given, the model's default saver directory is  
        used (unless file specifies the entire path).

        Args:
            directory: Optional checkpoint directory.
            file: Optional checkpoint file, or path if directory not given.
        ")
                        (reparse-symbol indented_block_body) [10466 10989])
                    ("from_spec" function
                       (:typemodifiers ("static")
                        :decorators 
                          ( ("staticmethod" function (:type "decorator") nil nil))                          
                        :arguments 
                          ( ("spec" variable nil (reparse-symbol function_parameters) [11026 11030])
                            ("kwargs" variable nil (reparse-symbol function_parameters) [11032 11038]))                          
                        :documentation "
        Creates an agent from a specification dict.
        ")
                        (reparse-symbol indented_block_body) [10994 11327]))                  
                :type "class")
                nil [998 11327]))          
      :file "agent.py"
      :pointmax 11327
      :fsize 11326
      :lastmodtime '(23097 19507 0 0)
      :unmatched-syntax nil)
    (semanticdb-table "naf_agent.py"
      :major-mode 'python-mode
      :tags 
        '( ("__future__" include nil nil [681 719])
            ("__future__" include nil nil [720 757])
            ("__future__" include nil nil [758 789])
            ("tensorforce" include nil nil [791 831])
            ("tensorforce.agents" include nil nil [832 874])
            ("tensorforce.models" include nil nil [875 915])
            ("NAFAgent" type
               (:documentation "
    Normalized Advantage Functions (NAF) for continuous DQN: https://arxiv.org/abs/1603.00748

    "
                :superclasses ("MemoryAgent")
                :members 
                  ( ("__init__" function
                       (:suite 
                          ( ("\"\"\"
        Creates a NAF-agent which is DQN-variant for continuous actions:
        https://arxiv.org/abs/1603.00748

        Args:
            states_spec: Dict containing at least one state definition. In the case of a single state,
               keys `shape` and `type` are necessary. For multiple states, pass a dict of dicts where each state
               is a dict itself with a unique name as its key.
            actions_spec: Dict containing at least one action definition. Actions have types and either `num_actions`
                for discrete actions or a `shape` for continuous actions. Consult documentation and tests for more.
            network_spec: List of layers specifying a neural network via layer types, sizes and optional arguments
                such as activation or regularisation. Full examples are in the examples/configs folder.
            device: Device string specifying model device.
            session_config: optional tf.ConfigProto with additional desired session configurations
            scope: TensorFlow scope, defaults to agent name (e.g. `dqn`).
            saver_spec: Dict specifying automated saving. Use `directory` to specify where checkpoints are saved. Use
                either `seconds` or `steps` to specify how often the model should be saved. The `load` flag specifies
                if a model is initially loaded (set to True) from a file `file`.
            summary_spec: Dict specifying summaries for TensorBoard. Requires a 'directory' to store summaries, `steps`
                or `seconds` to specify how often to save summaries, and a list of `labels` to indicate which values
                to export, e.g. `losses`, `variables`. Consult neural network class and model for all available labels.
            distributed_spec: Dict specifying distributed functionality. Use `parameter_server` and `replica_model`
                Boolean flags to indicate workers and parameter servers. Use a `cluster_spec` key to pass a TensorFlow
                cluster spec.
            optimizer: Dict specifying optimizer type and its optional parameters, typically a `learning_rate`.
                Available optimizer types include standard TensorFlow optimizers, `natural_gradient`,
                and `evolutionary`. Consult the optimizer test or example configurations for more.
            discount: Float specifying reward discount factor.
            variable_noise: Experimental optional parameter specifying variable noise (NoisyNet).
            states_preprocessing_spec: Optional list of states preprocessors to apply to state  
                (e.g. `image_resize`, `grayscale`).
            explorations_spec: Optional dict specifying action exploration type (epsilon greedy  
                or Gaussian noise).
            reward_preprocessing_spec: Optional dict specifying reward preprocessing.
            distributions_spec: Optional dict specifying action distributions to override default distribution choices.
                Must match action names.
            entropy_regularization: Optional positive float specifying an entropy regularization value.
            target_sync_frequency: Interval between optimization calls synchronizing the target network.
            target_update_weight: Update weight, 1.0 meaning a full assignment to target network from training network.
            huber_loss: Optional flat specifying Huber-loss clipping.
            batched_observe: Optional int specifying how many observe calls are batched into one session run.
                Without batching, throughput will be lower because every `observe` triggers a session invocation to
                update rewards in the graph.
            batch_size: Int specifying batch size used to sample from memory. Should be smaller than memory size.
            memory: Dict describing memory via `type` (e.g. `replay`) and `capacity`.
            first_update: Int describing at which time step the first update is performed. Should be larger
                than batch size.
            update_frequency: Int specifying number of observe steps to perform until an update is executed.
            repeat_update: Int specifying how many update steps are performed per update, where each update step implies
                sampling a batch from the memory and passing it to the model.
        \"\"\"" code nil (reparse-symbol indented_block_body) [1863 6233])
                            ("if" code nil (reparse-symbol indented_block_body) [6242 6331])
                            ("if" code nil (reparse-symbol indented_block_body) [6340 6528])
                            ("if" code nil (reparse-symbol indented_block_body) [6536 6706])
                            ("self" variable nil (reparse-symbol indented_block_body) [6715 6747])
                            ("self" variable nil (reparse-symbol indented_block_body) [6756 6776])
                            ("self" variable nil (reparse-symbol indented_block_body) [6785 6821])
                            ("self" variable nil (reparse-symbol indented_block_body) [6830 6848])
                            ("self" variable nil (reparse-symbol indented_block_body) [6857 6885])
                            ("self" variable nil (reparse-symbol indented_block_body) [6894 6926])
                            ("self" variable nil (reparse-symbol indented_block_body) [6935 6975])
                            ("self" variable nil (reparse-symbol indented_block_body) [6984 7008])
                            ("self" variable nil (reparse-symbol indented_block_body) [7017 7053])
                            ("self" variable nil (reparse-symbol indented_block_body) [7062 7120])
                            ("self" variable nil (reparse-symbol indented_block_body) [7129 7171])
                            ("self" variable nil (reparse-symbol indented_block_body) [7180 7238])
                            ("self" variable nil (reparse-symbol indented_block_body) [7247 7291])
                            ("self" variable nil (reparse-symbol indented_block_body) [7300 7352])
                            ("self" variable nil (reparse-symbol indented_block_body) [7361 7411])
                            ("self" variable nil (reparse-symbol indented_block_body) [7420 7468])
                            ("self" variable nil (reparse-symbol indented_block_body) [7477 7513])
                            ("self" variable nil (reparse-symbol indented_block_body) [7522 7550])
                            ("super" code nil (reparse-symbol indented_block_body) [7560 7910]))                          
                        :parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [1085 1089])
                            ("states_spec" variable nil (reparse-symbol function_parameters) [1099 1110])
                            ("actions_spec" variable nil (reparse-symbol function_parameters) [1120 1132])
                            ("network_spec" variable nil (reparse-symbol function_parameters) [1142 1154])
                            ("device" variable nil (reparse-symbol function_parameters) [1164 1170])
                            ("session_config" variable nil (reparse-symbol function_parameters) [1185 1199])
                            ("scope" variable nil (reparse-symbol function_parameters) [1214 1219])
                            ("saver_spec" variable nil (reparse-symbol function_parameters) [1235 1245])
                            ("summary_spec" variable nil (reparse-symbol function_parameters) [1260 1272])
                            ("distributed_spec" variable nil (reparse-symbol function_parameters) [1287 1303])
                            ("optimizer" variable nil (reparse-symbol function_parameters) [1318 1327])
                            ("discount" variable nil (reparse-symbol function_parameters) [1342 1350])
                            ("variable_noise" variable nil (reparse-symbol function_parameters) [1365 1379])
                            ("states_preprocessing_spec" variable nil (reparse-symbol function_parameters) [1394 1419])
                            ("explorations_spec" variable nil (reparse-symbol function_parameters) [1434 1451])
                            ("reward_preprocessing_spec" variable nil (reparse-symbol function_parameters) [1466 1491])
                            ("distributions_spec" variable nil (reparse-symbol function_parameters) [1506 1524])
                            ("entropy_regularization" variable nil (reparse-symbol function_parameters) [1539 1561])
                            ("target_sync_frequency" variable nil (reparse-symbol function_parameters) [1576 1597])
                            ("target_update_weight" variable nil (reparse-symbol function_parameters) [1613 1633])
                            ("double_q_model" variable nil (reparse-symbol function_parameters) [1647 1661])
                            ("huber_loss" variable nil (reparse-symbol function_parameters) [1677 1687])
                            ("batched_observe" variable nil (reparse-symbol function_parameters) [1702 1717])
                            ("batch_size" variable nil (reparse-symbol function_parameters) [1732 1742])
                            ("memory" variable nil (reparse-symbol function_parameters) [1755 1761])
                            ("first_update" variable nil (reparse-symbol function_parameters) [1776 1788])
                            ("update_frequency" variable nil (reparse-symbol function_parameters) [1804 1820])
                            ("repeat_update" variable nil (reparse-symbol function_parameters) [1832 1845]))                          
                        :documentation "
        Creates a NAF-agent which is DQN-variant for continuous actions:
        https://arxiv.org/abs/1603.00748

        Args:
            states_spec: Dict containing at least one state definition. In the case of a single state,
               keys `shape` and `type` are necessary. For multiple states, pass a dict of dicts where each state
               is a dict itself with a unique name as its key.
            actions_spec: Dict containing at least one action definition. Actions have types and either `num_actions`
                for discrete actions or a `shape` for continuous actions. Consult documentation and tests for more.
            network_spec: List of layers specifying a neural network via layer types, sizes and optional arguments
                such as activation or regularisation. Full examples are in the examples/configs folder.
            device: Device string specifying model device.
            session_config: optional tf.ConfigProto with additional desired session configurations
            scope: TensorFlow scope, defaults to agent name (e.g. `dqn`).
            saver_spec: Dict specifying automated saving. Use `directory` to specify where checkpoints are saved. Use
                either `seconds` or `steps` to specify how often the model should be saved. The `load` flag specifies
                if a model is initially loaded (set to True) from a file `file`.
            summary_spec: Dict specifying summaries for TensorBoard. Requires a 'directory' to store summaries, `steps`
                or `seconds` to specify how often to save summaries, and a list of `labels` to indicate which values
                to export, e.g. `losses`, `variables`. Consult neural network class and model for all available labels.
            distributed_spec: Dict specifying distributed functionality. Use `parameter_server` and `replica_model`
                Boolean flags to indicate workers and parameter servers. Use a `cluster_spec` key to pass a TensorFlow
                cluster spec.
            optimizer: Dict specifying optimizer type and its optional parameters, typically a `learning_rate`.
                Available optimizer types include standard TensorFlow optimizers, `natural_gradient`,
                and `evolutionary`. Consult the optimizer test or example configurations for more.
            discount: Float specifying reward discount factor.
            variable_noise: Experimental optional parameter specifying variable noise (NoisyNet).
            states_preprocessing_spec: Optional list of states preprocessors to apply to state  
                (e.g. `image_resize`, `grayscale`).
            explorations_spec: Optional dict specifying action exploration type (epsilon greedy  
                or Gaussian noise).
            reward_preprocessing_spec: Optional dict specifying reward preprocessing.
            distributions_spec: Optional dict specifying action distributions to override default distribution choices.
                Must match action names.
            entropy_regularization: Optional positive float specifying an entropy regularization value.
            target_sync_frequency: Interval between optimization calls synchronizing the target network.
            target_update_weight: Update weight, 1.0 meaning a full assignment to target network from training network.
            huber_loss: Optional flat specifying Huber-loss clipping.
            batched_observe: Optional int specifying how many observe calls are batched into one session run.
                Without batching, throughput will be lower because every `observe` triggers a session invocation to
                update rewards in the graph.
            batch_size: Int specifying batch size used to sample from memory. Should be smaller than memory size.
            memory: Dict describing memory via `type` (e.g. `replay`) and `capacity`.
            first_update: Int describing at which time step the first update is performed. Should be larger
                than batch size.
            update_frequency: Int specifying number of observe steps to perform until an update is executed.
            repeat_update: Int specifying how many update steps are performed per update, where each update step implies
                sampling a batch from the memory and passing it to the model.
        "
                        :constructor-flag t)
                        (reparse-symbol indented_block_body) [1063 7911])
                    ("initialize_model" function
                       (:parent "dummy"
                        :arguments 
                          ( ("self" variable nil (reparse-symbol function_parameters) [7937 7941]))                          )
                        (reparse-symbol indented_block_body) [7916 9079]))                  
                :type "class")
                nil [918 9079]))          
      :file "naf_agent.py"
      :pointmax 9079
      :fsize 9078
      :lastmodtime '(23097 11177 0 0)
      :unmatched-syntax nil))
  :file "!Users!xiaoli!Google Drive!PHD_research!projects!RLFPS!external_libs!tensorforce!tensorforce!agents!semantic.cache"
  :semantic-tag-version "2.0"
  :semanticdb-version "2.2")
